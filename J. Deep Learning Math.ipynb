{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48efc8a5",
   "metadata": {},
   "source": [
    "<img src=\"Images/atom.png\" alt=\"Atom\" style=\"width:60px\" align=\"left\" vertical-align=\"middle\">\n",
    "\n",
    "## 1. Introduction to Deep Learning Math\n",
    "*in Machine Learning*\n",
    "\n",
    "----\n",
    "Before we dive into creating our deep learning models, let’s take a step back and unbox the mechanisms of these models. In this lesson, we will investigate the foundations that run through the inner workings of neural networks. Hopefully, after reading about the steps our data takes on its deep learning journey, you will have a clearer picture of the overall process and feel ready to get your hands on some code!\n",
    "\n",
    "<br/>We are not going to assume that you have a deep understanding of linear algebra, and you will not need a high-level math background to follow along.\n",
    "\n",
    "<br/>Let’s take a look at the cartoon in the learning environment. Deep learning is commonly viewed as a jumbled mess of linear algebra which this cartoon pokes fun at.\n",
    "\n",
    "<img src=\"Images/machine_learning_cartoon.png\" style=\"width:500px\">\n",
    "\n",
    "<br/>In this lesson, we hope to clarify the fundamental concepts that make deep learning possible, so you can build your own learning models with more precision than “pour the data into this big pile of linear algebra.”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6685c9",
   "metadata": {},
   "source": [
    "<img src=\"Images/atom.png\" alt=\"Atom\" style=\"width:60px\" align=\"left\" vertical-align=\"middle\">\n",
    "\n",
    "## 2. Scalars, Vectors, and Matrices\n",
    "*in Machine Learning*\n",
    "\n",
    "----\n",
    "To start, let us go over a couple of topics that will be integral to understanding the mathematical operations that are present in deep learning, including how data is represented:\n",
    "\n",
    "<br/>*A. Scalar:* a single quantity that you can think of as a number. In machine learning models, we can use scalar quantities to manipulate data, and we often modify them to improve our model’s accuracy. We can also represent data as scalar values depending on what dataset we are working with. Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3ec1849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "x = 5\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3118f9",
   "metadata": {},
   "source": [
    "*B. Vector:* array of numbers. In Python, we often denote vectors as `NumPy` arrays. Each value in the array can be identified by its index (location within the array). Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32ed1e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.array([1,2,3])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e35854",
   "metadata": {},
   "source": [
    "*C. Matrices:* Matrices are grids of information with rows and columns. We can index a matrix just like an array; however, when indexing on a matrix, we need two arguments: one for the row and one for the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "792062ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.array([[1,2,3],[4,5,6],[7,8,9]])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce2e981",
   "metadata": {},
   "source": [
    "Scalar, vector, matrix:\n",
    "<img src=\"Images/Matrix-reference-diagram.png\" style=\"width:500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5403171",
   "metadata": {},
   "source": [
    "<img src=\"Images/atom.png\" alt=\"Atom\" style=\"width:60px\" align=\"left\" vertical-align=\"middle\">\n",
    "\n",
    "## 3. Tensors\n",
    "*in Machine Learning*\n",
    "\n",
    "----\n",
    "*Scalars, vectors,* and *matrices* are foundational objects in linear algebra. Understanding the different ways they interact with each other and can be manipulated through matrix algebra is integral before diving into deep learning. This is because the data structure we use in deep learning is called a *tensor,* which is a generalized form of a vector and matrix: a multidimensional array.\n",
    "\n",
    "<br/>A tensor allows for more flexibility with the type of data you are using and how you can manipulate that data.\n",
    "\n",
    "<br/>*Example:*\n",
    "<br/>Let us visualize what they look like and how they can be expressed using `NumPy` arrays. Use the applet below to get a feel for a tensor’s structure and how it is represented!\n",
    "\n",
    "<br/>In this applet, you are given boxes where you can fill in negative and positive whole numbers. On the right side is a set of nested arrays. These nested arrays form a 3-dimensional tensor that can be viewed as a “stack” or “layer” of grids, as shown on the left.\n",
    "\n",
    "<br/>The *shape* of this tensor is (3, 2, 5), as outlined on the diagram. The shape of our data is an important factor when we are feeding it into our neural network. It affects the way our model interacts with our inputs. This is something you will see in future lessons!\n",
    "\n",
    "<br/>If you would like to read more about tensors and see more ways to visualize tensors, feel free to do so [here!](https://www.tensorflow.org/guide/tensor)\n",
    "<img src=\"Images/tensor-applet.png\" style=\"width:800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc85d46",
   "metadata": {},
   "source": [
    "<img src=\"Images/atom.png\" alt=\"Atom\" style=\"width:60px\" align=\"left\" vertical-align=\"middle\">\n",
    "\n",
    "## 4. Matrix Algebra\n",
    "*in Machine Learning*\n",
    "\n",
    "----\n",
    "The following gifs walkthrough matrix multiplication, addition, and transpose. You can perform element-wise operations on tensors using matrix algebra as well, which you can read more about [here.](https://en.wikipedia.org/wiki/Matrix_(mathematics)#Addition,_scalar_multiplication,_and_transposition)\n",
    "\n",
    "<br/>*A. Matrix Addition:*\n",
    "<img src=\"Images/Matrix_B_v2.gif\" style=\"width:500px\">\n",
    "\n",
    "<br/>*B. Scalar Multiplication:*\n",
    "<img src=\"Images/Matrix_A.webp\" style=\"width:500px\">\n",
    "\n",
    "<br/>*C. Matrix Multiplication:* This is the most complicated, so spend time understanding how it is done.\n",
    "<img src=\"Images/ezgif-5-33c6f1bf16.gif\" style=\"width:500px\">\n",
    "\n",
    "<br/>*D. Transpose:*\n",
    "<img src=\"Images/Matrix_C.webp\" style=\"width:500px\">\n",
    "\n",
    "<br/>This is all of the matrix algebra we need to proceed with the rest of our deep learning investigation! These concepts are the fundamental building blocks of why deep learning models are so powerful. When we are training our models, we are performing operations on tensors. This data is analyzed, manipulated, and shaped by the matrix algebra we have quickly gone over. Below are some optional practice questions for you to try out Matrix algebra on your own!\n",
    "\n",
    "<br/>*Examples:*\n",
    "<img src=\"Images/Matrix_Examples.png\" style=\"width:300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1758d90",
   "metadata": {},
   "source": [
    "<img src=\"Images/atom.png\" alt=\"Atom\" style=\"width:60px\" align=\"left\" vertical-align=\"middle\">\n",
    "\n",
    "## 5. Neural Networks Concept Overview\n",
    "*in Machine Learning*\n",
    "\n",
    "----\n",
    "Let’s take a look at the journey our *inputs* take inside of a neural network! By an input, we mean a data point from our dataset. Our input can have many different features, so in our input layer, each node represents a different input feature. For example, if we were working with a dataset of different types of food, some of our features might be size, shape, nutrition, etc., where the value for each of these features would be held in an input node.\n",
    "\n",
    "<br/>Besides an input layer, our neural network has two other different types of layers:\n",
    "- *Hidden layers* are layers that come between the input layer and the output layer. They introduce complexity into our neural network and help with the learning process. You can have as many hidden layers as you want in a neural network (including zero of them).\n",
    "- The *output layer* is the final layer in our neural network. It produces the final result, so every neural network must have only one output layer.\n",
    "\n",
    "<br/>Each layer in a neural network contains nodes. Nodes between each layer are connected by *weights.* These are the learning parameters of our neural network, determining the strength of the connection between each linked node.\n",
    "\n",
    "<br/>The weighted sum between nodes and weights is calculated between each layer. For example, from our input layer, we take the weighted sum of the inputs and our weights with the following equation:\n",
    "> $ weighted\\_sum=(inputs⋅weight\\_transpose)+bias $\n",
    "\n",
    "<br/>We then apply an *activation function* to it.\n",
    "> $ Activation(weighted\\_sum) $\n",
    "\n",
    "<br/>The two formulas we have gone over take all the inputs through one layer of a neural network. Aside from the activation function, all of the transformations we have done so far are linear. Activation functions introduce nonlinearity in our learning model, creating more complexity during the learning process.\n",
    "\n",
    "<br/>This is what makes activation functions important. A neural network with many hidden layers but no activation functions would just be a series of successive layers that would be no more effective or accurate than simple linear regression.\n",
    "\n",
    "<br/>An activation function decides what is fired to the next neuron based on its calculation for the weighted sums. Various types of activation functions can be applied at each layer. The most popular one for hidden layers is *ReLU.*\n",
    "<img src=\"Images/ReLU.svg\" style=\"width:500px\">\n",
    "\n",
    "<br/>Others commonly used, often for the output layer, are *sigmoid* and *softmax.* You will learn more about these functions as you use them later in this course.\n",
    "<img src=\"Images/Sigmoid.svg\" style=\"width:500px\">\n",
    "\n",
    "<br/>*Summary:*\n",
    "<br/>In this diagram, we see a basic neural network with no hidden layers.\n",
    "<img src=\"Images/DL_static_2.png\" style=\"width:800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9a2450",
   "metadata": {},
   "source": [
    "<img src=\"Images/atom.png\" alt=\"Atom\" style=\"width:60px\" align=\"left\" vertical-align=\"middle\">\n",
    "\n",
    "## 6. The Math Behind the Journey!\n",
    "*in Machine Learning*\n",
    "\n",
    "----\n",
    "Let’s bring all of these concepts together and see how they function in a neural network with one hidden layer. As you scroll over each section, you will see the inputs/weights/calculations associated with it and see how inputs get from the starting point and make their way to the end!\n",
    "\n",
    "<br/>The process we have been going through is known as *forward propagation.* Inputs are moved forward from the input layer through the hidden layer(s) until they reach the output layer.\n",
    "<img src=\"Images/DL_static_1.png\" style=\"width:800px\">\n",
    "\n",
    "> A. Inputs layer: inputs are represented as vectors.\n",
    "\n",
    "<img src=\"Images/Neuron_inputs.png\" style=\"width:700px\">\n",
    "\n",
    "> B. Weights layer: we see how each set of weights (blue and yellow) is represented as a vector. When brought together, they make up the `weights_matrix` and `weights_matrix_transpose`.\n",
    "\n",
    "<img src=\"Images/Neuron_weights.png\" style=\"width:1000px\">\n",
    "\n",
    "> C. Weighted sum: observe the hidden nodes sections, you will notice that there are two parts. In the first step, we take the weighted sum of our data using the `weights_matrix_transpose`.\n",
    "\n",
    "<img src=\"Images/Neuron_weighted_sum.png\" style=\"width:1000px\">\n",
    "\n",
    "> D. Activation function: From the above weighted sum, we end up with a vector and apply our ReLU activation function to it.\n",
    "\n",
    "<img src=\"Images/Neuron_activation_function.png\" style=\"width:700px\">\n",
    "\n",
    "> E. Hidden (teal) weights: This takes us to our teal weights. These are represented as a vector. The `weights_teal_transpose` turns our *row* vector into a *column* vector. \n",
    "\n",
    "<img src=\"Images/Neuron_weights_hidden.png\" style=\"width:500px\">\n",
    "\n",
    "> F. Hidden weighted sum: Then we take another weighted sum in our output layer, this time between our `hidden_nodes` and our `weights_teal_transpose`.\n",
    "\n",
    "<img src=\"Images/Neuron_weighted_sum_hidden.png\" style=\"width:1000px\">\n",
    "\n",
    "> G. Activation function: Following this, we have a sigmoid activation function, which gives us our output.\n",
    "\n",
    "<img src=\"Images/Neuron_activation_function_hidden.png\" style=\"width:500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f125c2",
   "metadata": {},
   "source": [
    "<img src=\"Images/atom.png\" alt=\"Atom\" style=\"width:60px\" align=\"left\" vertical-align=\"middle\">\n",
    "\n",
    "## 7. Loss Functions\n",
    "*in Machine Learning*\n",
    "\n",
    "----\n",
    "We have seen how we get to an output! Now, what do we do with it? When a value is outputted, we calculate its error using a loss function. Our predicted values are compared with the actual values within the training data. There are two commonly used loss calculation formulas:\n",
    "- **Mean squared error,** which is most likely familiar to you if you have come across *linear regression.* This gif below shows how mean squared error is calculated for a line of best fit in linear regression.\n",
    "<img src=\"Images/Loss.gif\" style=\"width:800px\">\n",
    "- **Cross-entropy loss,** which is used for classification learning models rather than regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f697265f",
   "metadata": {},
   "source": [
    "<img src=\"Images/atom.png\" alt=\"Atom\" style=\"width:60px\" align=\"left\" vertical-align=\"middle\">\n",
    "\n",
    "## 8. Backpropagation\n",
    "*in Machine Learning*\n",
    "\n",
    "----\n",
    "This all seems fine and dandy so far. However, what if our output values are inaccurate? Do we cry? Try harder next time? Well, we can do that, but the good news is that there is more to our deep learning models.\n",
    "\n",
    "<br/>This is where *backpropagation* and *gradient descent* come into play. Forward propagation deals with feeding the input values through hidden layers to the final output layer. *Backpropagation* refers to the computation of gradients with an algorithm known as *gradient descent*. This algorithm continuously updates and refines the weights between neurons to minimize our loss function.\n",
    "\n",
    "<br/>By gradient, we mean the rate of change with respect to the parameters of our loss function. From this, backpropagation determines how much each weight is contributing to the error in our loss function, and gradient descent will update our weight values accordingly to decrease this error.\n",
    "\n",
    "<br/>This is a conceptual overview of backpropagation. If you would like to engage with the gritty mathematics of it, you can do so [here.](https://en.wikipedia.org/wiki/Backpropagation) However, for this tutorial, we will not need this level of detailed understanding.\n",
    "\n",
    "<br/>*Example:*\n",
    "<br/>Gradient descent updates the weights in each iteration to decrease the error. \n",
    "<br/><img src=\"Images/Backpropagation.png\" style=\"width:800px\"> <img src=\"Images/Backpropagation2.png\" style=\"width:800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246b2617",
   "metadata": {},
   "source": [
    "<img src=\"Images/atom.png\" alt=\"Atom\" style=\"width:60px\" align=\"left\" vertical-align=\"middle\">\n",
    "\n",
    "## 9. Gradient Descent\n",
    "*in Machine Learning*\n",
    "\n",
    "----\n",
    "We have the overall process of backpropagation down! Now, let’s zoom in on what is happening during gradient descent.\n",
    "\n",
    "<br/>If we think about the concept graphically, we want to look for the minimum point of our loss function because this will yield us the highest accuracy. If we start at a random point on our loss function, gradient descent will take “steps” in the “downhill direction” towards the negative gradient. The size of the “step” taken is dependent on our learning rate. Choosing the optimal learning rate is important because it affects both the efficiency and accuracy of our results.\n",
    "\n",
    "<br/>The formula used with learning rate to update our weight parameters is the following:\n",
    "> $ parameter\\_new=parameter\\_old+learning\\_rate⋅gradient(loss\\_function(parameter\\_old)) $\n",
    "\n",
    "<br/>The learning rate we choose affects how large the “steps” our pointer takes when trying to optimize our error function. Initial intuition might indicate that you should choose a large learning rate; however, as shown above, this can lead you to overshoot the value we are looking for and cause a divergent search.\n",
    "\n",
    "<br/>Now you might think that you should choose an incredibly small learning rate; however, if it is too small, it could cause your model to be unbearably inefficient or get stuck in a local minimum and never find the optimum value. It is a tricky game of finding the correct combination of efficiency and accuracy.\n",
    "\n",
    "<br/>Take a look at the graphs, which depict some common issues you may run into when selecting a learning rate. If you select a learning rate that is too small, it will take a very long time to find the minimum value. However, if you choose a learning rate that is too large, it might overshoot the minimum value and end up with a divergent algorithm\n",
    "\n",
    "<br/>Choosing an ideal learning rate means it should find the ideal loss value efficiently and accurately.\n",
    "<img src=\"Images/Gradient_descent.gif\" style=\"width:800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3320040f",
   "metadata": {},
   "source": [
    "<img src=\"Images/atom.png\" alt=\"Atom\" style=\"width:60px\" align=\"left\" vertical-align=\"middle\">\n",
    "\n",
    "## 10. Stochastic Gradient Descent\n",
    "*in Machine Learning*\n",
    "\n",
    "----\n",
    "This leads us to the final point about gradient descent. In deep learning models, we are often dealing with extremely large datasets. Because of this, performing backpropagation and gradient descent calculations on all of our data may be inefficient and computationally exhaustive no matter what learning rate we choose.\n",
    "\n",
    "<br/>To solve this problem, a variation of gradient descent known as *Stochastic Gradient Descent (SGD)* was developed. Let’s say we have 100,000 data points and 5 parameters. If we did 1000 iterations (also known as *epochs* in Deep Learning) we would end up with 100000⋅5⋅1000 = 500,000,000 computations. We do not want our computer to do that many computations on top of the rest of the learning model; it will take forever.\n",
    "\n",
    "<br/>This is where SGD comes to play. Instead of performing gradient descent on our entire dataset, we pick out a random data point to use at each iteration. This cuts back on computation time immensely while still yielding accurate results.\n",
    "\n",
    "<br/>The diagram below shows the performance differences between SGD and GD. You may notice that the SGD graph is a bit more sporadic. There is a reason for this, and we will address it in the next exercise.\n",
    "\n",
    "<br/>The main point is that both will reach the ideal parameters, and SGD will be easier and more efficient for your computer processor. Because of this, SGD is almost universally used in favor of normal GD.\n",
    "\n",
    "<br/>However, as well will see next, there are even more variants of gradient descent.\n",
    "<img src=\"Images/GD-Diagram.png\" style=\"width:800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44123bbc",
   "metadata": {},
   "source": [
    "<img src=\"Images/atom.png\" alt=\"Atom\" style=\"width:60px\" align=\"left\" vertical-align=\"middle\">\n",
    "\n",
    "## 11. More Variants of Gradient Descent\n",
    "*in Machine Learning*\n",
    "\n",
    "----\n",
    "Just when you thought SDG solved all our problems, even more options come into the picture!\n",
    "\n",
    "<br/>There are also other variants of gradient descent such as *Adam optimization algorithm* and *mini-batch gradient descent*. *Adam* is an adaptive learning algorithm that finds individual learning rates for each parameter. *Mini-batch gradient descent* is similar to SGD except instead of iterating on one data point at a time, we iterate on small batches of fixed size.\n",
    "\n",
    "<br/>Adam optimizer’s ability to have an adaptive learning rate has made it an ideal variant of gradient descent and is commonly used in deep learning models. Mini-batch gradient descent was developed as an ideal trade-off between GD and SGD. Since mini-batch does not depend on just one training sample, it has a much smoother curve and is less affected by outliers and noisy data making it a more optimal algorithm for gradient descent than SGD.\n",
    "\n",
    "<br/>These are just some quick notes! You can read more about Adam [here](https://arxiv.org/abs/1412.6980) and more about mini-batch [here.](https://arxiv.org/pdf/1609.04747.pdf) Experts in deep learning are constantly coming up with ways to improve these algorithms to make them more efficient and accurate, so the ability to adapt and build upon what you learn as you dive into this domain will be key!\n",
    "\n",
    "<br/>This is a diagram you will become accustomed to in future lessons. It shows us our loss function performance over many *epochs* (iterations) of our deep learning model with different types of gradient descent. These graphs are extremely useful when creating learning models because they offer us a detailed view of their performance.\n",
    "<img src=\"Images/Variants-of-Gradient Descent.png\" style=\"width:800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11564cd",
   "metadata": {},
   "source": [
    "<img src=\"Images/atom.png\" alt=\"Atom\" style=\"width:60px\" align=\"left\" vertical-align=\"middle\">\n",
    "\n",
    "## 12. Review\n",
    "*in Machine Learning*\n",
    "\n",
    "----\n",
    "This overview completes the necessary mathematical intuition you need to move forward and begin coding your own learning models! To recap all the things we have learned (so many things!!!):\n",
    "\n",
    "<br/>Scalars, vectors, matrices, and tensors\n",
    "- A *scalar* is a singular quantity like a number.\n",
    "- A *vector* is an array of numbers (scalar values).\n",
    "- A *matrix* is a grid of information with rows and columns.\n",
    "- A *tensor* is a multidimensional array and is a generalized version of a vector and matrix.\n",
    "\n",
    "<br/>Matrix Algebra\n",
    "- In *scalar multiplication,* every entry of the matrix is multiplied by a scalar value.\n",
    "- In *matrix addition,* corresponding matrix entries are added together.\n",
    "- In *matrix multiplication,* the dot product between the corresponding rows of the first matrix and columns of the second matrix is calculated.\n",
    "- A *matrix transpose* turns the rows of a matrix into columns.\n",
    "\n",
    "<br/>In *forward propagation,* data is sent through a neural network to get initial outputs and error values.\n",
    "- *Weights* are the learning parameters of a deep learning model that determine the strength of the connection between two nodes.\n",
    "- A *bias node* shifts the activation function either left or right to create the best fit for the given data in a deep learning model.\n",
    "- *Activation Functions* are used in each layer of a neural network and determine whether neurons should be “fired” or not based on output from a weighted sum.\n",
    "- *Loss functions* are used to calculate the error between the predicted values and actual values of our training set in a learning model.\n",
    "\n",
    "<br/>In *backpropagation,* the gradient of the loss function is calculated with respect to the weight parameters within a neural network.\n",
    "- *Gradient descent* updates our weight parameters by iteratively minimizing our loss function to increase our model’s accuracy.\n",
    "- *Stochastic gradient descent* is a variant of gradient descent, where instead of using all data points to update parameters, a random data point is selected.\n",
    "- *Adam optimization* is a variant of SGD that allows for adaptive learning rates.\n",
    "- *Mini-batch gradient descent* is a variant of GD that uses random batches of data to update parameters instead of a random datapoint.\n",
    "\n",
    "<br/>This has been a ton of material, so do not fret if you are confused or still wrapping your head around some of the concepts. Many of this will be reviewed as you begin coding models. Get excited to see how these concepts are powered with TensorFlow!\n",
    "<img src=\"Images/review_comic.png\" style=\"width:500px\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
