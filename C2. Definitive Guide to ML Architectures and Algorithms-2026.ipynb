{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "764a7b2f-9e82-4771-8f61-04e07ad88c98",
   "metadata": {},
   "source": [
    "<img src=\"Images/atom.png\" alt=\"Atom\" style=\"width:60px\" align=\"left\" vertical-align=\"middle\">\n",
    "\n",
    "# **The Definitive Guide to Machine Learning Architectures and Algorithms**\n",
    "*What is Machine Learning? IBM Think*\n",
    "\n",
    "----\n",
    "Machine Learning (ML) is the field of study that gives computers the ability to learn without being explicitly programmed. It is categorized by the **learning paradigm** (how the system learns) and the **data structure** (whether the data has labels/answers or not).\n",
    "\n",
    "Below is the classification of ML into five primary paradigms:\n",
    "1.  **Supervised Learning** (Task-Driven: \"Here is the data and the answer; learn the relationship.\")\n",
    "2.  **Unsupervised Learning** (Data-Driven: \"Here is data; find the hidden structure.\")\n",
    "3.  **Reinforcement Learning** (Environment-Driven: \"Maximize your score by trial and error.\")\n",
    "4.  **Deep Learning** (Neural Representation: \"Learn features automatically using layers.\")\n",
    "5.  **Hybrid Paradigms** (Semi-Supervised, Self-Supervised, Generative)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08262c80-9503-4c8c-9d0d-5ba6b64531f1",
   "metadata": {},
   "source": [
    "<img src=\"Images/atom.png\" alt=\"Atom\" style=\"width:60px\" align=\"left\" vertical-align=\"middle\">\n",
    "\n",
    "## **1. Supervised Learning**\n",
    "*What is Machine Learning? IBM Think*\n",
    "\n",
    "----\n",
    "**Detailed Definition:**\n",
    "Supervised learning is the most common type of ML. Imagine a teacher supervising a student; the teacher provides the student with practice problems (Input $X$) and the correct answers (Output $Y$). The goal of the algorithm is to learn a mapping function ($f$) such that $Y = f(X)$. The model adjusts its internal parameters until its predictions match the provided answers as closely as possible.\n",
    "\n",
    "**Sub-types:**\n",
    "*   **Regression:** Predicting a continuous number (e.g., What will the temperature be tomorrow?).\n",
    "*   **Classification:** Predicting a label or category (e.g., Is this email Spam or Safe?)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5171149f-eee8-4b37-a955-9fd268725cf8",
   "metadata": {},
   "source": [
    "### **A. Linear Regression (Regression)**\n",
    "*   **Detailed Summary:**\n",
    "    Linear Regression is the \"workhorse\" of statistics. It assumes a linear relationship between input variables (x) and the single output variable (y). Ideally, if you plot the data points on a graph, this algorithm attempts to draw a straight line that passes as close to all points as possible. It does this by minimizing the \"residuals\"—the vertical distance between the actual data points and the line.\n",
    "*   **Use Cases:** Sales forecasting, housing price prediction, risk analysis.\n",
    "*   **Pseudo-code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a953d2-b378-4a8b-bf2a-adb1ad157df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Initialize weights (slope 'w') and bias (intercept 'b') to zero\n",
    "Set learning_rate (alpha)\n",
    "Loop for N epochs:\n",
    "    1. Predict output: y_pred = (w * Input) + b\n",
    "    2. Calculate Error: (y_pred - Actual_Y)\n",
    "    3. Update w: w = w - alpha * gradient(Error)\n",
    "    4. Update b: b = b - alpha * gradient(Error)\n",
    "Return final line equation y = wx + b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88e3015-0927-4934-8532-44b69fd42efd",
   "metadata": {},
   "source": [
    "*   **Reference:** *Friedman, J., Hastie, T., & Tibshirani, R. (2001). The Elements of Statistical Learning.*\n",
    "*   **Problems & Flaws:**\n",
    "    *   **Linearity Assumption:** It fails if the relationship between data is curved (non-linear).\n",
    "    *   **Sensitive to Outliers:** A single extreme data point can skew the entire line significantly.\n",
    "    *   **Multicollinearity:** If input features are highly correlated with each other, the model becomes unstable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87f6b13-5e47-411e-9809-76273a153bde",
   "metadata": {},
   "source": [
    "### **B. Logistic Regression (Classification)**\n",
    "*   **Detailed Summary:**\n",
    "    Despite the name \"Regression,\" this is used for classification. Instead of fitting a straight line, it fits an \"S\" shaped curve (the Sigmoid function). It calculates the weighted sum of inputs and \"squashes\" the result between 0 and 1. This value represents the **probability** that a data point belongs to a specific class (e.g., 0.85 means an 85% chance it is spam).\n",
    "*   **Use Cases:** Credit default prediction (Yes/No), Disease diagnosis (Positive/Negative).\n",
    "*   **Pseudo-code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43eabc9-0493-4f16-aed8-1dbbcc9b79df",
   "metadata": {},
   "outputs": [],
   "source": [
    "Initialize weights\n",
    "Loop for N epochs:\n",
    "    1. Calculate weighted sum: z = (w * Input) + b\n",
    "    2. Apply Sigmoid function: probability = 1 / (1 + e^-z)\n",
    "    3. Calculate Log Loss (difference between probability and Actual_Label)\n",
    "    4. Update weights using Gradient Descent to minimize Loss\n",
    "Prediction Rule: If probability > 0.5 return Class 1, else Class 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31954b63-c537-4051-ab2a-96f71292c99c",
   "metadata": {},
   "source": [
    "*   **Reference:** *Cox, D. R. (1958). The regression analysis of binary sequences. Journal of the Royal Statistical Society.*\n",
    "*   **Problems & Flaws:**\n",
    "    *   **Linear Decision Boundary:** It assumes the classes can be separated by a straight line (or plane). It struggles with complex, non-linear patterns.\n",
    "    *   **Feature Engineering:** Requires the user to identify strictly relevant independent variables; irrelevant features confuse the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46811cc6-5410-4504-9ced-8c517b01b2e5",
   "metadata": {},
   "source": [
    "### **C. Support Vector Machines - SVM (Classification/Regression)**\n",
    "*   **Detailed Summary:**\n",
    "    SVM is a powerful algorithm that tries to find the widest possible \"street\" (margin) between two categories of data. It draws a decision boundary (hyperplane) so that the distance to the nearest data points of each class (the \"support vectors\") is maximized. If data cannot be separated linearly, SVM uses a \"Kernel Trick\" to project data into a higher dimension (3D or more) where separation becomes possible.\n",
    "*   **Use Cases:** Handwriting recognition, facial detection, protein structure prediction.\n",
    "*   **Pseudo-code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff58391-1687-45ec-b218-179172520ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "For dataset with two classes:\n",
    "    Find hyperplane equation (w * x + b = 0)\n",
    "    Maximize: Distance (Margin) between hyperplane and nearest data points\n",
    "    Constraint: No data points allowed inside the margin (Hard Margin)\n",
    "                OR allow some errors (Soft Margin)\n",
    "To Predict (New_Point):\n",
    "    Value = w * New_Point + b\n",
    "    If Value >= 0 return Class A\n",
    "    Else return Class B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1662bc03-bfe9-4a91-a1d6-a43766327d0c",
   "metadata": {},
   "source": [
    "*   **Reference:** *Cortes, C., & Vapnik, V. (1995). Support-vector networks. Machine Learning.*\n",
    "*   **Problems & Flaws:**\n",
    "    *   **Scalability:** It is computationally very expensive and slow on large datasets.\n",
    "    *   **Noise Sensitivity:** If classes overlap significantly (noisy data), finding an optimal margin is difficult.\n",
    "    *   **Black Box (Kernels):** Choosing the right \"Kernel\" function (RBF, Polynomial, Linear) is often trial-and-error.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415eecf0-e465-481b-85df-79311a6651df",
   "metadata": {},
   "source": [
    "### **D. Random Forest (Ensemble / Bagging)**\n",
    "*   **Detailed Summary:**\n",
    "    This utilizes the \"Wisdom of Crowds.\" A single Decision Tree is often prone to error (overfitting). A Random Forest trains hundreds of independent decision trees. Crucially, each tree is trained on a random subset of the data (Bootstrapping) and only sees a random subset of features at each split. When making a prediction, the forest aggregates the answers: the majority vote wins.\n",
    "*   **Use Cases:** Customer churn prediction, fraud detection, high-dimensional genomic data analysis.\n",
    "*   **Pseudo-code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2377d59-5125-4d59-b997-41dd30c9583a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Function TrainForest(Dataset, N_Trees):\n",
    "    For i from 1 to N_Trees:\n",
    "        1. Create a \"Bootstrap Sample\" (random selection with replacement)\n",
    "        2. Train a Decision Tree on this sample\n",
    "        3. At each node split, consider only a random subset of features\n",
    "    Store all N trees\n",
    "\n",
    "Function Predict(Input):\n",
    "    Get predictions from all N trees\n",
    "    If Classification: Return Majority Vote\n",
    "    If Regression: Return Average of all predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01db4099-c058-4ab6-a59f-2d2da7985003",
   "metadata": {},
   "source": [
    "*   **Reference:** *Breiman, L. (2001). Random Forests. Machine Learning.*\n",
    "*   **Problems & Flaws:**\n",
    "    *   **Complexity:** The model is essentially a \"black box\"—it is difficult to interpret exactly *why* it made a specific decision compared to a single tree.\n",
    "    *   **Prediction Speed:** While training can be parallelized, making predictions is slow because data must pass through every single tree.\n",
    "    *   **Memory:** Storing hundreds of trees requires significant memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcce8f7b-6c7f-45e7-bbf3-1158e65f3c61",
   "metadata": {},
   "source": [
    "### **E. Gradient Boosting - XGBoost/LightGBM (Ensemble / Boosting)**\n",
    "*   **Detailed Summary:**\n",
    "    Like Random Forest, this uses many trees. However, instead of building them independently, it builds them **sequentially**. The first tree makes predictions, and the algorithm calculates the errors (residuals). The second tree is trained specifically to fix the errors of the first tree. This process repeats, with each new tree correcting the previous ones, resulting in a highly accurate \"additive\" model.\n",
    "*   **Use Cases:** Web search ranking, win-rate prediction in sports, Kaggle competition winners.\n",
    "*   **Pseudo-code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95aa3e9c-bdcd-4dfc-8e62-92b29793ec0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Initialize model with a constant value (e.g., average of target)\n",
    "Loop for N_Trees:\n",
    "    1. Calculate \"Residuals\" (Actual_Y - Current_Model_Prediction)\n",
    "    2. Train a new shallow Decision Tree to predict these Residuals\n",
    "    3. Update Model = Old_Model + (Learning_Rate * New_Tree_Prediction)\n",
    "Return Final Additive Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd3f725-4dd1-4bd6-acfe-e659fafa8302",
   "metadata": {},
   "source": [
    "*   **Reference:** *Friedman, J. H. (2001). Greedy function approximation: A gradient boosting machine.*\n",
    "*   **Problems & Flaws:**\n",
    "    *   **Overfitting:** Because it obsessively tries to correct errors, it can easily memorize noise in the data if the \"learning rate\" is too high.\n",
    "    *   **Outliers:** It is very sensitive to outliers because the algorithm treats them as \"large errors\" and focuses too much attention on fixing them.\n",
    "    *   **Training Speed:** Trees are built one after another, so training cannot be easily parallelized (unlike Random Forest)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b93d423-55bc-473c-becf-550465e8338b",
   "metadata": {},
   "source": [
    "<img src=\"Images/atom.png\" alt=\"Atom\" style=\"width:60px\" align=\"left\" vertical-align=\"middle\">\n",
    "\n",
    "## **2. Unsupervised Learning**\n",
    "*What is Machine Learning? IBM Think*\n",
    "\n",
    "----\n",
    "**Detailed Definition:**\n",
    "In Unsupervised Learning, the data has no labels. The algorithm is given a massive dump of data and left to figure it out. It looks for statistical patterns, groupings, or structures. It answers the question: \"How is this data organized?\"\n",
    "\n",
    "**Sub-types:**\n",
    "*   **Clustering:** Grouping similar items together.\n",
    "*   **Dimensionality Reduction:** Compressing data while keeping relevant information.\n",
    "*   **Association:** Finding rules/relationships between items."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca2dec4-9760-4ab5-b133-ea69c77b01cc",
   "metadata": {},
   "source": [
    "### **F. K-Means Clustering (Clustering)**\n",
    "*   **Detailed Summary:**\n",
    "    K-Means partitions a dataset into $K$ distinct, non-overlapping subgroups (clusters). It starts by randomly placing $K$ center points (centroids). It then assigns every data point to the nearest centroid. Once assigned, it calculates the average (mean) of the points in that cluster and moves the centroid to that center. It repeats this until the centroids stop moving.\n",
    "*   **Use Cases:** Customer segmentation, image compression (color quantization).\n",
    "*   **Pseudo-code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7497bb-af65-4c48-9e90-17a22ccf1c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Select K random points as initial Centroids\n",
    "Loop until Centroids do not move (Convergence):\n",
    "    1. Assignment Step: For each data point, calculate distance to all Centroids.\n",
    "       Assign the point to the cluster of the nearest Centroid.\n",
    "    2. Update Step: Calculate the mean average position of all points in a cluster.\n",
    "       Move the Centroid to this new mean."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672de15c-0cfd-434a-8bf3-ab9298035a68",
   "metadata": {},
   "source": [
    "*   **Reference:** *MacQueen, J. (1967). Some methods for classification and analysis of multivariate observations.*\n",
    "*   **Problems & Flaws:**\n",
    "    *   **Fixed K:** You must define the number of clusters ($K$) *before* you start. If you guess wrong, the results are meaningless.\n",
    "    *   **Initialization:** If the random starting points are bad, the algorithm might find suboptimal clusters (local minima).\n",
    "    *   **Shape:** It assumes clusters are spherical and essentially the same size. It fails with irregular shapes (e.g., a crescent shape)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1577e9-0e47-405c-a6d7-a0c031d00634",
   "metadata": {},
   "source": [
    "### **G. Principal Component Analysis - PCA (Dimensionality Reduction)**\n",
    "*   **Detailed Summary:**\n",
    "    Real-world data often has too many variables (high dimensionality), making it hard to visualize or process. PCA reduces the number of variables by mathematically \"squashing\" the data onto a new coordinate system. It creates new variables (Principal Components) that are composites of the old ones, prioritizing the directions where the data varies the most (where the information is).\n",
    "*   **Use Cases:** Data visualization, facial recognition (Eigenfaces), reducing noise in signals.\n",
    "*   **Pseudo-code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e89dc8-af12-4889-8c5c-f0a6d89af216",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Standardize the data (Scale to Mean = 0, Variance = 1)\n",
    "2. Compute the Covariance Matrix of features (How features vary with each other)\n",
    "3. Calculate Eigenvalues and Eigenvectors of the matrix\n",
    "4. Sort Eigenvectors by Eigenvalues (High to Low importance)\n",
    "5. Choose top N Eigenvectors (Principal Components)\n",
    "6. Transform original data by multiplying with this matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce38bb2-228a-4266-8a56-4190c237e467",
   "metadata": {},
   "source": [
    "*   **Reference:** *Pearson, K. (1901). On lines and planes of closest fit to systems of points in space.*\n",
    "*   **Problems & Flaws:**\n",
    "    *   **Interpretability:** The new \"Principal Components\" are complex mixtures of the original features. You lose the ability to say \"Age caused this\" because the variable is now *\"0.5 * Age + 0.3 * Income\"*.\n",
    "    *   **Information Loss:** By reducing dimensions, you inevitably throw away some data.\n",
    "    *   **Linear:** It assumes relationships between variables are linear."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5ff017-af5f-4957-a968-ca80c8782463",
   "metadata": {},
   "source": [
    "### **H. Apriori (Association Rule Learning)**\n",
    "*   **Detailed Summary:**\n",
    "    Apriori is used for mining frequent itemsets. It operates on a simple logic: if a set of items (e.g., Beer, Chips) is bought frequently, then the individual items (Beer) and (Chips) must also be frequent. It scans the database to find individual items that meet a minimum threshold, then combines them into pairs, then triplets, pruning any combinations that don't meet the threshold.\n",
    "*   **Use Cases:** Market Basket Analysis (\"People who bought Diapers also bought Beer\"), recommendation systems.\n",
    "*   **Pseudo-code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c77cf0-14c5-475f-bbca-e36aeb278be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Set minimum support threshold (e.g., item must appear in 5% of transactions)\n",
    "1. Find all individual items that appear > 5% of the time (L1)\n",
    "2. Join L1 items to make pairs (L2). \n",
    "3. Prune: Remove any pairs where the individual items weren't in L1.\n",
    "4. Count frequency of pairs. Keep only those > 5%.\n",
    "5. Repeat (make triplets from pairs) until no more frequent itemsets exist.\n",
    "6. Generate rules (If A -> Then B) from these sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e31c6f3-750e-46a4-aa59-05603e9a7e77",
   "metadata": {},
   "source": [
    "*   **Reference:** *Agrawal, R., & Srikant, R. (1994). Fast algorithms for mining association rules.*\n",
    "*   **Problems & Flaws:**\n",
    "    *   **Computationally Expensive:** On large datasets with many items, checking every combination is incredibly slow.\n",
    "    *   **Spurious Correlations:** It might find associations that are coincidental rather than causal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad7f96f-7d97-419c-af17-6ca9c155b2e4",
   "metadata": {},
   "source": [
    "<img src=\"Images/atom.png\" alt=\"Atom\" style=\"width:60px\" align=\"left\" vertical-align=\"middle\">\n",
    "\n",
    "## **3. Reinforcement Learning (RL)**\n",
    "*What is Machine Learning? IBM Think*\n",
    "\n",
    "----\n",
    "**Detailed Definition:**\n",
    "RL is about learning from interaction. An \"Agent\" exists in an \"Environment.\" The agent takes an action, and the environment responds with a new state and a **reward** (positive score) or **penalty** (negative score). The agent's goal is to learn a policy (strategy) that maximizes the total cumulative reward over time. It is similar to training a dog with treats."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fee985-083f-4b33-a57b-eb3221062269",
   "metadata": {},
   "source": [
    "### **I. Q-Learning (Model-Free RL)**\n",
    "*   **Detailed Summary:**\n",
    "    Q-Learning is a value-based algorithm. The agent creates a \"cheat sheet\" (Q-Table) that lists every possible state and every possible action. The values in the table (Q-values) represent the \"quality\" or expected future reward of taking that action. Initially, the agent guesses randomly, but over time, it updates the table based on the rewards it actually receives.\n",
    "*   **Use Cases:** Game playing (Pac-Man, Chess), Robot navigation, Traffic light control.\n",
    "*   **Pseudo-code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2936b16f-2125-4ac8-8277-1ea6db921617",
   "metadata": {},
   "outputs": [],
   "source": [
    "Initialize Q-Table with zeros\n",
    "Loop for each Episode:\n",
    "    Reset State S\n",
    "    Loop until Episode ends:\n",
    "        1. Choose Action A (Exploration vs Exploitation):\n",
    "           - Randomly (explore) OR\n",
    "           - Best value in Q-Table (exploit)\n",
    "        2. Take Action A, observe Reward R and New State S'\n",
    "        3. Update Q-Table using Bellman Equation:\n",
    "           Q[S,A] = Q[S,A] + alpha * (R + gamma * max(Q[S', all_actions]) - Q[S,A])\n",
    "        4. S = S'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5dfc67-7e35-4af8-835f-58eca9b73948",
   "metadata": {},
   "source": [
    "*   **Reference:** *Watkins, C. J., & Dayan, P. (1992). Q-learning. Machine Learning.*\n",
    "*   **Problems & Flaws:**\n",
    "    *   **Curse of Dimensionality:** If the environment is complex (like a real-world robot), the Q-Table becomes too massive to store in memory.\n",
    "    *   **Slow Convergence:** It takes millions of trial-and-error attempts to learn a good policy.\n",
    "    *   **Exploration/Exploitation Dilemma:** Balancing trying new things (which might fail) vs. sticking to what works (which might be suboptimal) is difficult."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf639fae-07e3-408c-91b0-455c960507e8",
   "metadata": {},
   "source": [
    "<img src=\"Images/atom.png\" alt=\"Atom\" style=\"width:60px\" align=\"left\" vertical-align=\"middle\">\n",
    "\n",
    "## **4. Deep Learning (Neural Networks)**\n",
    "*What is Machine Learning? IBM Think*\n",
    "\n",
    "----\n",
    "**Detailed Definition:**\n",
    "Deep Learning is a specialized subset of ML inspired by the biological brain. It uses **Artificial Neural Networks (ANNs)** with many (\"deep\") layers between input and output. Unlike traditional ML, where humans must tell the computer what features to look for (e.g., \"count the number of corners\"), Deep Learning performs **Feature Extraction** automatically. It learns simple patterns in early layers and combines them into complex concepts in deeper layers.\n",
    "\n",
    "**Sub-types:**\n",
    "1. Feedforward Neural Networks (FNN)\n",
    "2. Recurrent Neural Networks (RNN)\n",
    "3. Convolutional Neural Networks (CNN)\n",
    "4. Transformer Architectures\n",
    "5. Generative Models\n",
    "6. Graph Neural Networks (GNN)\n",
    "7. Self-Organizing Maps (SOM / Kohonen Networks)\n",
    "8. Spiking Neural Networks (SNN)\n",
    "9. Modular Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f821f09-0bf1-42cd-9df2-9020d9a6176e",
   "metadata": {},
   "source": [
    "### **J. Convolutional Neural Networks - CNN (Vision)**\n",
    "*   **Detailed Summary:**\n",
    "    CNNs are designed specifically for grid-like data, such as images. Instead of looking at an image as a flat list of pixels, a CNN uses \"filters\" (kernels) that slide over the image like a magnifying glass. These filters learn to detect spatial features—first edges, then textures, then shapes, and finally full objects (like a face or a car). It preserves the spatial relationship between pixels.\n",
    "*   **Use Cases:** Medical Image Diagnosis (X-Ray/MRI), Autonomous Vehicles, Face Recognition.\n",
    "*   **Pseudo-code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f90d98-2e88-49b0-88cf-2a1b777730f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Function ForwardPass(Image):\n",
    "    1. Convolution Layer: Slide learnable filters over image -> Generate Feature Maps\n",
    "    2. ReLU Activation: Apply max(0, x) to introduce non-linearity\n",
    "    3. Pooling Layer: Downsample (e.g., Max Pooling) to reduce image size/computation\n",
    "    4. Repeat steps 1-3 multiple times (Deep layers)\n",
    "    5. Flatten: Convert 2D feature maps to a long 1D vector\n",
    "    6. Fully Connected Layer: Process vector through dense neurons\n",
    "    7. Output Layer: Softmax function to classify probability of each object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447d332e-5790-4e4e-8540-daa033bb91a8",
   "metadata": {},
   "source": [
    "*   **Reference:** *LeCun, Y., et al. (1998). Gradient-based learning applied to document recognition.*\n",
    "*   **Problems & Flaws:**\n",
    "    *   **Data Hungry:** Requires tens of thousands of labeled images to perform well.\n",
    "    *   **Computational Cost:** Training requires high-end GPUs and massive electricity consumption.\n",
    "    *   **Adversarial Attacks:** Changing a few pixels (invisible to humans) can trick the CNN into thinking a panda is a gibbon."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c54e67-c758-4592-ac43-67141b8b1e7e",
   "metadata": {},
   "source": [
    "### **K. Transformers (NLP / Generative AI)**\n",
    "*   **Detailed Summary:**\n",
    "    The Transformer architecture revolutionized Natural Language Processing (NLP). Previous models read text sequentially (left to right), which made them forget the beginning of long sentences. Transformers use a mechanism called **\"Self-Attention.\"** This allows the model to look at the entire sentence at once and calculate how much every word relates to every other word (e.g., understanding that \"bank\" refers to money, not a river, based on the context of the word \"deposit\").\n",
    "*   **Use Cases:** Language Translation, Text Summarization, Chatbots (ChatGPT), Code Generation.\n",
    "*   **Pseudo-code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d687e77-daff-43fd-8361-087f1f7c292b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Function SelfAttention(Query, Key, Value):\n",
    "    1. Calculate Similarity = DotProduct(Query, Key)\n",
    "    2. Scale values and apply Softmax -> Attention Weights\n",
    "    3. Multiply Weights by Value -> Context Vector (Weighted meaning of word)\n",
    "\n",
    "Training Loop:\n",
    "    1. Embed Input Text into vectors + Add Positional Encoding\n",
    "    2. Pass through Multi-Head Attention blocks (Parallel processing)\n",
    "    3. Feed-Forward Networks\n",
    "    4. Calculate Loss (predict next word or translation)\n",
    "    5. Backpropagation to update weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5090f1-7812-4c7c-8728-70761db00647",
   "metadata": {},
   "source": [
    "*   **Reference:** *Vaswani, A., et al. (2017). Attention is all you need. NeurIPS.*\n",
    "*   **Problems & Flaws:**\n",
    "    *   **Quadratic Complexity:** As the text gets longer, the memory required grows squarely ($N^2$), limiting the amount of text it can read at once (context window).\n",
    "    *   **Hallucination:** Because they are probabilistic predictors, they can confidently state false information as fact.\n",
    "    *   **Black Box:** It is nearly impossible to trace exactly *why* a Transformer generated a specific sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f136a0-4ef8-4978-bc04-8d527a0e0c2e",
   "metadata": {},
   "source": [
    "<img src=\"Images/atom.png\" alt=\"Atom\" style=\"width:60px\" align=\"left\" vertical-align=\"middle\">\n",
    "\n",
    "## **5. Hybrid Paradigms (Semi-Supervised & Generative)**\n",
    "*What is Machine Learning? IBM Think*\n",
    "\n",
    "----\n",
    "**Detailed Definition:**\n",
    "These paradigms address data limitations or specific generation tasks.\n",
    "*   **Semi-Supervised:** Uses a small amount of labeled data to guide the learning of a large amount of unlabeled data.\n",
    "*   **Generative:** Instead of classifying data, these models learn to create *new* data that looks like the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba74250-6b71-4a7c-950b-98971bbcc44f",
   "metadata": {},
   "source": [
    "### **L. Generative Adversarial Networks - GANs (Generative)**\n",
    "*   **Detailed Summary:**\n",
    "    GANs consist of two neural networks locked in a competitive game (zero-sum).\n",
    "    1.  The **Generator**: Tries to create fake data (e.g., an image of a person) that looks real.\n",
    "    2.  The **Discriminator**: Tries to distinguish between real images from the dataset and fake images from the Generator.\n",
    "    As they train, the Discriminator gets better at spotting fakes, forcing the Generator to create hyper-realistic fakes.\n",
    "*   **Use Cases:** DeepFakes, Image Upscaling (Super-Resolution), Drug Discovery.\n",
    "*   **Pseudo-code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e765f7e8-b88d-4064-b5d1-360d5ecf00de",
   "metadata": {},
   "outputs": [],
   "source": [
    "Loop for N epochs:\n",
    "    1. Generator creates Fake_Image from random noise vector\n",
    "    2. Discriminator receives batch of Real_Images and Fake_Images\n",
    "    3. Discriminator Loss: Penalty for incorrectly classifying Real vs Fake\n",
    "    4. Update Discriminator weights (Gradient Descent)\n",
    "    5. Generator Loss: Penalty if Discriminator correctly identified Fake\n",
    "    6. Update Generator weights (Gradient Ascent - maximize Discriminator error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b389ba-7b5e-4426-8bd5-5cfd2942e90b",
   "metadata": {},
   "source": [
    "*   **Reference:** *Goodfellow, I., et al. (2014). Generative adversarial nets. NeurIPS.*\n",
    "*   **Problems & Flaws:**\n",
    "    *   **Mode Collapse:** The Generator might find one specific image that fools the discriminator and produces *only* that image over and over, losing diversity.\n",
    "    *   **Non-Convergence:** The two networks might oscillate forever without reaching a stable solution, making training very difficult and unstable.\n",
    "    *   **Vanishing Gradients:** If the Discriminator is too good, the Generator stops learning because it gets no useful feedback."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
