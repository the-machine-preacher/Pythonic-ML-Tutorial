{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bd8218c",
   "metadata": {},
   "source": [
    "<img src=\"Images/atom.png\" alt=\"Atom\" style=\"width:60px\" align=\"left\" vertical-align=\"middle\">\n",
    "\n",
    "## 1. Introduction\n",
    "*in Machine Learning*\n",
    "\n",
    "----\n",
    "A neural network, just like any machine learning method, learns how to perform tasks by processing data and adjusting its model to best predict the desired outcome. Most popular machine learning tasks are:\n",
    "- *Classification:* given data and true labels or categories for each data point, train a model that predicts for each data example what its label should be. For example, given data of previous fire hazards, our model can learn how to predict whether a fire will occur for a given day in the future, with all the factors taken into account.\n",
    "- *Regression:* given data and true continuous value for each data point, train a model that can predict values for each data example. For example, given the previous stock market data, we can build a regression model that forecasts what the stock market price will be at a specific point in time when the data is available.\n",
    "\n",
    "<br/>Parametric models such as neural networks are described by *parameters:* configuration variables representing the model’s knowledge. We can tweak the parameters using the training data and we can evaluate the performance of the model using hold-out test data the model has not seen during training.\n",
    "\n",
    "<br/>Take a look at the main components of a neural network learning pipeline depicted below:\n",
    "- *Input data:* this is used to train a neural network model you need to provide it with some training data.\n",
    "- *An optimizer:* this is an algorithm that based on the training data adjusts the parameters of the network in order to perform the task at hand.\n",
    "- *Loss or cost function:* this informs the optimizer whether it is doing a good job on the training data and how to adjust the parameters in the right direction.\n",
    "- *Evaluation metrics:* these tell us how well the current model performs on validation data. For example, mean absolute error for regression tells us how far the predictions are on average from the true values.\n",
    "<img src=\"Images/introduction_diagram.png\" style=\"width:1000px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2638b46c",
   "metadata": {},
   "source": [
    "<img src=\"Images/atom.png\" alt=\"Atom\" style=\"width:60px\" align=\"left\" vertical-align=\"middle\">\n",
    "\n",
    "## 2. Predicting medical costs: loading the data\n",
    "*in Machine Learning*\n",
    "\n",
    "----\n",
    "Every machine learning pipeline starts with data and a task. Let’s take a look at the [Medical Cost Personal Datasets dataset](https://www.kaggle.com/datasets/mirichoi0218/insurance), which consists of seven columns with the following descriptions:\n",
    "<img src=\"Images/insurance_data.png\" style=\"width:800px\">\n",
    "\n",
    "<br/>We would like to predict the individual medical costs (charges) given the rest of the columns/features. Since charges represent continuous values (in dollars), we’re performing a regression task. \n",
    "\n",
    "<br/>Our data is in the `.csv` format and we load it with pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b87e1e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   age     sex     bmi  children smoker     region      charges\n",
      "0   19  female  27.900         0    yes  southwest  16884.92400\n",
      "1   18    male  33.770         1     no  southeast   1725.55230\n",
      "2   28    male  33.000         3     no  southeast   4449.46200\n",
      "3   33    male  22.705         0     no  northwest  21984.47061\n",
      "4   32    male  28.880         0     no  northwest   3866.85520\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dataset = pd.read_csv('Data/insurance.csv')\n",
    "#view the first 5 entries of the dataset\n",
    "print(dataset.head()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e89be4",
   "metadata": {},
   "source": [
    "Next, we split the data into *features* (independent variables) and the *target* variable (dependent variable):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9bcad04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe slicing using iloc, this designates the first 6 rows as features (independent variables)\n",
    "features = dataset.iloc[:,0:6]  \n",
    "# We select the last column with -1, which is designated as target variables (dependent variable)\n",
    "labels = dataset.iloc[:,-1] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de7b789",
   "metadata": {},
   "source": [
    "The pandas `shape` property tells us the shape of our data — a vector of two values: the number of samples and the number of features. To check the shape of our dataset, we can do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38802fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1338, 6)\n"
     ]
    }
   ],
   "source": [
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a213bba",
   "metadata": {},
   "source": [
    "Or, to make things clearer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59b4d4bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features:  6\n",
      "Number of samples:  1338\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of features: \", features.shape[1])\n",
    "print(\"Number of samples: \", features.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f777aafb",
   "metadata": {},
   "source": [
    "To see a useful summary statistics of the dataset we do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "329e90e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               age          bmi     children\n",
      "count  1338.000000  1338.000000  1338.000000\n",
      "mean     39.207025    30.663397     1.094918\n",
      "std      14.049960     6.098187     1.205493\n",
      "min      18.000000    15.960000     0.000000\n",
      "25%      27.000000    26.296250     0.000000\n",
      "50%      39.000000    30.400000     1.000000\n",
      "75%      51.000000    34.693750     2.000000\n",
      "max      64.000000    53.130000     5.000000\n"
     ]
    }
   ],
   "source": [
    "print(features.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6719bb1",
   "metadata": {},
   "source": [
    "*Exercise:*\n",
    "1. Use the `.shape` property of pandas DataFrames to print the number of samples of the `labels` Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f939ac1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1338,)\n"
     ]
    }
   ],
   "source": [
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006e0f10",
   "metadata": {},
   "source": [
    "2. Use the `.describe()` method to print the summary statistics of the `labels` Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fff2f999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count     1338.000000\n",
      "mean     13270.422265\n",
      "std      12110.011237\n",
      "min       1121.873900\n",
      "25%       4740.287150\n",
      "50%       9382.033000\n",
      "75%      16639.912515\n",
      "max      63770.428010\n",
      "Name: charges, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(labels.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037300fa",
   "metadata": {},
   "source": [
    "<img src=\"Images/atom.png\" alt=\"Atom\" style=\"width:60px\" align=\"left\" vertical-align=\"middle\">\n",
    "\n",
    "## 3. Data preprocessing: one-hot encoding and standardization\n",
    "*in Machine Learning*\n",
    "\n",
    "----\n",
    "### A. One-hot encoding of categorical features:\n",
    "Since neural networks cannot work with string data directly, we need to convert our categorical features (“region”) into numerical. *One-hot encoding* creates a binary column for each category. For example, since the “region” variable has four categories, the one-hot encoding will result in four binary columns: “northeast”, “northwest”, “southeast”, “southwest” as shown in the table below.\n",
    "<img src=\"Images/one_hot_encoding.png\" style=\"width:500px\">\n",
    "\n",
    "<br/>One-hot encoding can be accomplished by using the pandas `get_dummies()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "037893f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.get_dummies(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690daebc",
   "metadata": {},
   "source": [
    "### B. Split data into train and test sets:\n",
    "In machine learning, we train a model on a training data, and we evaluate its performance on a held-out set of data, our test set, not seen during the learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36657772",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(features, labels, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efdf1af",
   "metadata": {},
   "source": [
    "Here we chose the test size to be 33% of the total data, and random state controls the shuffling applied to the data before applying the split."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee91514",
   "metadata": {},
   "source": [
    "### C. Standardize/normalize numerical features:\n",
    "The usual preprocessing step for numerical variables, among others, is *standardization* that rescales features to zero mean and unit variance. Why do we want to do that? Well, our features have different scales or units: “age” has an interval of [18, 64] and the “children” column’s interval is much smaller, [0, 5]. By having features with differing scales, the optimizer might update some weights faster than the others.\n",
    "\n",
    "<br/>*Normalization* is another way of preprocessing numerical data: it scales the numerical features to a fixed range - usually between 0 and 1.\n",
    "\n",
    "<br/>So which should you use? Well, there isn’t always a clear answer, but you can try them all out and choose the one method that gives the best results.\n",
    "\n",
    "<br/>To normalize the numerical features we use an exciting addition to `scikit-learn`, `ColumnTransformer`, in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2c224f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "ct = ColumnTransformer([('normalize', Normalizer(), ['age', 'bmi', 'children'])], remainder='passthrough')\n",
    "features_train_norm = ct.fit_transform(features_train)\n",
    "features_test_norm = ct.transform(features_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ceeb03",
   "metadata": {},
   "source": [
    "The name of the column transformer is “only numeric”, it applies a `Normalizer()` to the `age`, `bmi`, and `children` columns, and for the rest of the columns it just passes through. `ColumnTransformer()` returns `NumPy` arrays and we convert them back to a pandas DataFrame so we can see some useful summaries of the scaled data.\n",
    "\n",
    "<br/>To convert a NumPy array back into a pandas DataFrame, we can do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1970bb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train_norm = pd.DataFrame(features_train_norm, columns = features_train.columns)\n",
    "features_test_norm = pd.DataFrame(features_test_norm, columns = features_test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef8f175",
   "metadata": {},
   "source": [
    "Note that we fit the scaler to the training data only, and then we apply the trained scaler onto the test data. This way we avoid “information leakage” from the training set to the test set. These two datasets should be completely unaware of each other!\n",
    "\n",
    "<br/>*Exercise:*\n",
    "1. Create a new ColumnTransformer instance called `my_ct` that uses `StandardScaler()` and `'scale'` (instead of `Normalizer()` and `'normalize'`) with the same numerical features (`age`, `bmi`, `children`). Make sure to passthrough the remainder of the columns. I already imported the `StandardScaler` module for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34faff20",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_ct = ColumnTransformer([('scale', StandardScaler(), ['age', 'bmi', 'children'])], remainder='passthrough')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da162e38",
   "metadata": {},
   "source": [
    "2. Use the `.fit_transform()` method of `my_ct` to fit the column transformer to the `features_train` DataFrame and at the same time transform it. Assign the result to a variable called `features_train_scale`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57331135",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train_scale = my_ct.fit_transform(features_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d11759",
   "metadata": {},
   "source": [
    "3. Use the `.transform()` method to transform the trained column transformer `my_ct` to the `features_test` DataFrame. Assign the result to a variable called `features_test_scale`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f48faa40",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_test_scale = my_ct.transform(features_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1258efe",
   "metadata": {},
   "source": [
    "4. Transform the `features_train_scale` NumPy array back to a DataFrame using `pd.DataFrame()` and assign the result back to a variable called `features_train_scale`. For the `columns` attribute use the `.columns` property of `features_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e47da8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train_scale = pd.DataFrame(features_train_scale, columns = features_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ed8c12",
   "metadata": {},
   "source": [
    "5. Transform the `features_test_scale` NumPy array back to DataFrame using `pd.DataFrame()` and assign the result back to a variable called `features_test_scale`. For the `columns` attribute use the `.columns` property of `features_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "46cafebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_test_scale = pd.DataFrame(features_test_scale, columns = features_test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6aea2b",
   "metadata": {},
   "source": [
    "6. Print the statistics summary of the resulting train and test DataFrames, `features_train_scale` and `features_test_scale`. Observe the statistics of the numeric columns (mean, variance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ab8633b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                age           bmi      children  sex_female    sex_male  \\\n",
      "count  8.960000e+02  8.960000e+02  8.960000e+02  896.000000  896.000000   \n",
      "mean  -1.189525e-17  6.819941e-16 -3.965082e-17    0.487723    0.512277   \n",
      "std    1.000559e+00  1.000559e+00  1.000559e+00    0.500128    0.500128   \n",
      "min   -1.494934e+00 -2.438281e+00 -9.126072e-01    0.000000    0.000000   \n",
      "25%   -8.613199e-01 -7.139833e-01 -9.126072e-01    0.000000    0.000000   \n",
      "50%   -1.650038e-02 -5.227104e-02 -8.245892e-02    0.000000    1.000000   \n",
      "75%    8.987207e-01  6.598116e-01  7.476894e-01    1.000000    1.000000   \n",
      "max    1.743540e+00  3.776715e+00  3.238134e+00    1.000000    1.000000   \n",
      "\n",
      "        smoker_no  smoker_yes  region_northeast  region_northwest  \\\n",
      "count  896.000000  896.000000        896.000000        896.000000   \n",
      "mean     0.790179    0.209821          0.256696          0.252232   \n",
      "std      0.407408    0.407408          0.437054          0.434536   \n",
      "min      0.000000    0.000000          0.000000          0.000000   \n",
      "25%      1.000000    0.000000          0.000000          0.000000   \n",
      "50%      1.000000    0.000000          0.000000          0.000000   \n",
      "75%      1.000000    0.000000          1.000000          1.000000   \n",
      "max      1.000000    1.000000          1.000000          1.000000   \n",
      "\n",
      "       region_southeast  region_southwest  \n",
      "count        896.000000        896.000000  \n",
      "mean           0.255580          0.235491  \n",
      "std            0.436431          0.424542  \n",
      "min            0.000000          0.000000  \n",
      "25%            0.000000          0.000000  \n",
      "50%            0.000000          0.000000  \n",
      "75%            1.000000          0.000000  \n",
      "max            1.000000          1.000000  \n",
      "              age         bmi    children  sex_female    sex_male   smoker_no  \\\n",
      "count  442.000000  442.000000  442.000000  442.000000  442.000000  442.000000   \n",
      "mean    -0.005829    0.061133   -0.011089    0.509050    0.490950    0.805430   \n",
      "std      0.966688    1.057251    1.002194    0.500485    0.500485    0.396318   \n",
      "min     -1.494934   -2.295321   -0.912607    0.000000    0.000000    0.000000   \n",
      "25%     -0.931721   -0.706877   -0.912607    0.000000    0.000000    1.000000   \n",
      "50%     -0.016500    0.007923   -0.082459    1.000000    0.000000    1.000000   \n",
      "75%      0.810719    0.774556    0.747689    1.000000    1.000000    1.000000   \n",
      "max      1.743540    3.684752    3.238134    1.000000    1.000000    1.000000   \n",
      "\n",
      "       smoker_yes  region_northeast  region_northwest  region_southeast  \\\n",
      "count  442.000000         442.00000        442.000000        442.000000   \n",
      "mean     0.194570           0.21267          0.223982          0.305430   \n",
      "std      0.396318           0.40966          0.417382          0.461111   \n",
      "min      0.000000           0.00000          0.000000          0.000000   \n",
      "25%      0.000000           0.00000          0.000000          0.000000   \n",
      "50%      0.000000           0.00000          0.000000          0.000000   \n",
      "75%      0.000000           0.00000          0.000000          1.000000   \n",
      "max      1.000000           1.00000          1.000000          1.000000   \n",
      "\n",
      "       region_southwest  \n",
      "count        442.000000  \n",
      "mean           0.257919  \n",
      "std            0.437985  \n",
      "min            0.000000  \n",
      "25%            0.000000  \n",
      "50%            0.000000  \n",
      "75%            1.000000  \n",
      "max            1.000000  \n"
     ]
    }
   ],
   "source": [
    "print(features_train_scale.describe())\n",
    "print(features_test_scale.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c6736f",
   "metadata": {},
   "source": [
    "<img src=\"Images/atom.png\" alt=\"Atom\" style=\"width:60px\" align=\"left\" vertical-align=\"middle\">\n",
    "\n",
    "## 4. Neural network model: tf.keras.Sequential\n",
    "*in Machine Learning*\n",
    "\n",
    "----\n",
    "Now that we have our data preprocessed we can start building the neural network model. The most frequently used model in TensorFlow is Keras *Sequential*. A sequential model, as the name suggests, allows us to create models layer-by-layer in a step-by-step fashion. This model can have only one input tensor and only one output tensor.\n",
    "\n",
    "<br/>To design a sequential model, we first need to import `Sequential` from `keras.models`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b4e9cc0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Appuk\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ff198b",
   "metadata": {},
   "source": [
    "To improve readability, we will design the model in a separate Python function called `design_model()`. The following command initializes a Sequential model instance `my_model`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "20444eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def design_model(features):\n",
    "    model = Sequential(name=\"my first model\")\n",
    "    return model\n",
    "\n",
    "my_model = Sequential(name=\"my first model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fac2800",
   "metadata": {},
   "source": [
    "`name` is an optional argument to any model in Keras.\n",
    "\n",
    "<br/>Finally, we invoke our function in the main program with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c4ce2fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = design_model(features_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0121534f",
   "metadata": {},
   "source": [
    "The model’s `layers` are accessed via the layers attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e01b81fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(my_model.layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60afd744",
   "metadata": {},
   "source": [
    "As expected, the list of layers is empty. In the next exercise, we will start adding layers to our model.\n",
    "\n",
    "<br/>*Exercise:*\n",
    "1. In the `design_model()` function: initialize an instance of `Sequential()` and assign it to a variable called `model`. Then return the model instance `model` from the `design_model()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "87e227ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def design_model(features):\n",
    "    model = Sequential()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd234e8",
   "metadata": {},
   "source": [
    "2. In the main program, using the `layers` attribute, print the layers of the model instance `model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "95059194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Appuk\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv('Data/insurance.csv') # load the dataset\n",
    "features = dataset.iloc[:,0:6] # Choose first 7 columns as features\n",
    "labels = dataset.iloc[:,-1] # Choose the final column for prediction\n",
    "\n",
    "features = pd.get_dummies(features) # One-hot encoding for categorical variables\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(features, labels, test_size=0.33, random_state=42) # Split the data into training (67%) and test (33%) data\n",
    "\n",
    "# Standardize\n",
    "ct = ColumnTransformer([('standardize', StandardScaler(), ['age', 'bmi', 'children'])], remainder='passthrough')\n",
    "features_train = ct.fit_transform(features_train)\n",
    "features_test = ct.transform(features_test)\n",
    "\n",
    "# Invoke the function for our model design\n",
    "model = design_model(features_train)\n",
    "\n",
    "# Print the layers\n",
    "print(model.layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76babcbe",
   "metadata": {},
   "source": [
    "<img src=\"Images/atom.png\" alt=\"Atom\" style=\"width:60px\" align=\"left\" vertical-align=\"middle\">\n",
    "\n",
    "## 5. Neural network model: layers\n",
    "*in Machine Learning*\n",
    "\n",
    "----\n",
    "Layers are the building blocks of neural networks and can contain 1 or more neurons. Each layer is associated with parameters: weights, and bias, that are tuned during the learning. A fully-connected layer in which all neurons connect to all neurons in the next layer is created the following way in TensorFlow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8086895b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "# We chose 3 neurons here\n",
    "layer = layers.Dense(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e62610",
   "metadata": {},
   "source": [
    "This layer looks like this graphically:\n",
    "<img src=\"Images/layers_diagram.svg\" style=\"width:800px\">\n",
    "\n",
    "<br/>Pay attention to the dimensions of the weight and bias parameter matrices. Since we chose to create a layer with three neurons, the number of outputs of this layer is 3. Hence, the bias parameter would be a vector of (3, 1) dimensions. But what is the first dimension of the weights matrix? Without knowing how many features or input nodes are in the previous layer, we have no way of knowing! For that reason, with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7672b49e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(layer.weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6b239a",
   "metadata": {},
   "source": [
    "We get an empty array since no input layer is specified. However, if we write:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2a7eedb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'dense_1/kernel:0' shape=(11, 3) dtype=float32, numpy=\n",
      "array([[ 0.5956639 ,  0.5126096 ,  0.5785978 ],\n",
      "       [ 0.1813289 , -0.46000773,  0.4351108 ],\n",
      "       [ 0.49746227,  0.32362473,  0.1388455 ],\n",
      "       [ 0.30135095, -0.6340486 ,  0.4041859 ],\n",
      "       [ 0.3742993 , -0.02449501,  0.03209305],\n",
      "       [-0.09133315,  0.6518723 , -0.09996384],\n",
      "       [ 0.31146115, -0.34994245, -0.5222039 ],\n",
      "       [ 0.31551486, -0.59917915, -0.04777688],\n",
      "       [-0.08655363,  0.27628505, -0.44196674],\n",
      "       [ 0.45963335,  0.44013715, -0.26298845],\n",
      "       [-0.6037773 ,  0.41678703, -0.20069456]], dtype=float32)>, <tf.Variable 'dense_1/bias:0' shape=(3,) dtype=float32, numpy=array([0., 0., 0.], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "# 13388 samples, 11 features as in our dataset\n",
    "input = tf.ones((1338, 11))\n",
    "# A fully-connected layer with 3 neurons\n",
    "layer = layers.Dense(3)\n",
    "# Calculate the outputs\n",
    "output = layer(input)\n",
    "# Print the weights\n",
    "print(layer.weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f59b048",
   "metadata": {},
   "source": [
    "We get that the weight matrix has `shape = (11, 3)` and the bias matrix has `shape=(3,)`. Compare these weights with the diagram above to make sure you can associate the resulting shapes to it.\n",
    "\n",
    "<br/>Fortunately, we don’t have to worry about this. TensorFlow will determine the shapes of the weight matrix and bias matrix automatically the moment it encounters the first input.\n",
    "\n",
    "<br/>*Exercise:*\n",
    "1. Change the number of samples in the `input` tensor from 1338 to 5000. How does this change affect the shape of the weight and bias vectors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d965df90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'dense_2/kernel:0' shape=(11, 3) dtype=float32, numpy=\n",
      "array([[ 0.48526013, -0.03079021,  0.20876396],\n",
      "       [ 0.46494317, -0.17629805,  0.47531736],\n",
      "       [-0.04008222, -0.5142191 ,  0.5109639 ],\n",
      "       [-0.4639427 , -0.20485678,  0.06226873],\n",
      "       [ 0.00150168,  0.07655787,  0.20752954],\n",
      "       [ 0.34711754,  0.30834746,  0.3538165 ],\n",
      "       [-0.4878307 , -0.5460418 ,  0.5268897 ],\n",
      "       [-0.55937195,  0.54024696, -0.20742199],\n",
      "       [-0.29072195,  0.28665823, -0.12120688],\n",
      "       [-0.20033634, -0.03908801, -0.51500547],\n",
      "       [-0.5081461 ,  0.44304228, -0.08830643]], dtype=float32)>, <tf.Variable 'dense_2/bias:0' shape=(3,) dtype=float32, numpy=array([0., 0., 0.], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "# 13388 samples, 11 features as in our dataset\n",
    "input = tf.ones((5000, 11))\n",
    "# A fully-connected layer with 3 neurons\n",
    "layer = layers.Dense(3)\n",
    "# Calculate the outputs\n",
    "output = layer(input)\n",
    "# Print the weights\n",
    "print(layer.weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802773c5",
   "metadata": {},
   "source": [
    "2. Now, change the number of features in input from 11 to 21. How does this change affect the shape of the weight and bias vectors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a9a8fd29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'dense_3/kernel:0' shape=(21, 3) dtype=float32, numpy=\n",
      "array([[ 0.0862571 ,  0.41380572,  0.49643922],\n",
      "       [-0.05824327,  0.49197423,  0.20641243],\n",
      "       [ 0.21893144, -0.24017906, -0.43543732],\n",
      "       [ 0.14845884,  0.2034111 , -0.24715662],\n",
      "       [-0.49297845, -0.3737409 , -0.3927033 ],\n",
      "       [-0.24269176,  0.07486439, -0.3072039 ],\n",
      "       [-0.16982019, -0.47522783, -0.24347746],\n",
      "       [ 0.35459077, -0.17449772, -0.1040051 ],\n",
      "       [ 0.19645607,  0.07279503, -0.0255338 ],\n",
      "       [-0.3176787 , -0.27472854, -0.34658146],\n",
      "       [-0.03967273, -0.46081388,  0.12320101],\n",
      "       [-0.1772039 ,  0.20319295, -0.27084708],\n",
      "       [ 0.23490953, -0.25558138, -0.14912665],\n",
      "       [-0.31256497, -0.17188573,  0.2698233 ],\n",
      "       [-0.32975376,  0.39006472,  0.12028968],\n",
      "       [-0.36244082,  0.34840143,  0.25267684],\n",
      "       [ 0.14136767, -0.02619708, -0.29262817],\n",
      "       [-0.09993172, -0.23336887, -0.17277384],\n",
      "       [-0.47303462,  0.25248313, -0.04553413],\n",
      "       [ 0.16226089,  0.1768763 ,  0.43882406],\n",
      "       [-0.24163938,  0.3186549 , -0.21532643]], dtype=float32)>, <tf.Variable 'dense_3/bias:0' shape=(3,) dtype=float32, numpy=array([0., 0., 0.], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "# 13388 samples, 11 features as in our dataset\n",
    "input = tf.ones((5000, 21))\n",
    "# A fully-connected layer with 3 neurons\n",
    "layer = layers.Dense(3)\n",
    "# Calculate the outputs\n",
    "output = layer(input)\n",
    "# Print the weights\n",
    "print(layer.weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a76c7d",
   "metadata": {},
   "source": [
    "3. Change the number of neurons in `layer` (below where `input` is defined) from 3 to 10. How does this change affect the shape of the weight and bias vectors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1eb540d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'dense_4/kernel:0' shape=(21, 10) dtype=float32, numpy=\n",
      "array([[ 1.74634814e-01,  3.02489698e-02, -3.85932028e-02,\n",
      "         1.43705547e-01,  8.27887654e-03, -4.08338249e-01,\n",
      "        -2.80947387e-02, -3.60466987e-01,  1.32441461e-01,\n",
      "        -1.37825191e-01],\n",
      "       [-3.22229177e-01, -8.34048092e-02, -1.77137911e-01,\n",
      "        -4.04979229e-01, -3.28722179e-01, -3.20237398e-01,\n",
      "        -3.59046668e-01,  4.48316336e-02,  1.76968515e-01,\n",
      "        -3.92362416e-01],\n",
      "       [ 1.13584399e-02, -3.49011511e-01, -2.81731188e-01,\n",
      "         2.13400960e-01,  2.21962452e-01, -4.71977592e-02,\n",
      "         3.68815184e-01, -1.36629850e-01, -3.27083498e-01,\n",
      "         9.42098498e-02],\n",
      "       [-4.35595214e-01, -3.75793755e-01, -2.29545832e-02,\n",
      "        -3.77012670e-01, -2.21576750e-01,  2.92596519e-02,\n",
      "        -4.15438682e-01,  4.44765985e-02,  2.80404329e-01,\n",
      "        -4.21756119e-01],\n",
      "       [ 3.39443982e-01, -1.84488744e-01,  1.17498040e-02,\n",
      "        -3.09018254e-01, -1.77058518e-01,  2.53757596e-01,\n",
      "        -3.46655071e-01, -6.86925948e-02,  1.88440919e-01,\n",
      "         1.75170481e-01],\n",
      "       [ 2.32362509e-01,  2.49369621e-01, -5.52161932e-02,\n",
      "         3.08678389e-01, -9.76304114e-02, -3.69455129e-01,\n",
      "        -9.13996398e-02, -1.37275875e-01, -3.23839426e-01,\n",
      "        -6.25598729e-02],\n",
      "       [ 6.47068620e-02, -3.29062045e-01,  1.58948064e-01,\n",
      "        -1.46678448e-01,  4.34302866e-01, -2.81518251e-01,\n",
      "        -3.49294513e-01,  2.41482198e-01,  3.76909256e-01,\n",
      "         2.58260727e-01],\n",
      "       [ 2.70274878e-02, -2.72127986e-03, -4.01384562e-01,\n",
      "         2.54306972e-01, -3.93503845e-01,  4.08784449e-02,\n",
      "        -2.77265668e-01, -1.04421377e-02,  2.32298017e-01,\n",
      "        -4.32809353e-01],\n",
      "       [-3.28450829e-01,  3.38516116e-01, -1.99942842e-01,\n",
      "        -4.37142044e-01, -2.02780426e-01, -2.61792600e-01,\n",
      "         3.42296422e-01,  1.57390654e-01,  2.30413675e-03,\n",
      "        -4.31084037e-02],\n",
      "       [ 1.94327533e-01,  2.70531118e-01,  1.96207702e-01,\n",
      "        -1.41170442e-01, -3.35503817e-02,  1.33097231e-01,\n",
      "         1.39963806e-01, -1.27744079e-01, -1.55896932e-01,\n",
      "         9.49045420e-02],\n",
      "       [-3.87132168e-05,  4.00224328e-02,  1.75566256e-01,\n",
      "        -1.58688426e-03, -3.71645778e-01,  4.11328018e-01,\n",
      "        -2.52046615e-01, -3.90968531e-01, -3.47526073e-01,\n",
      "        -7.28482008e-03],\n",
      "       [ 1.10144436e-01, -3.96905541e-02,  1.60499632e-01,\n",
      "        -2.73746401e-01, -2.74562031e-01, -4.18914735e-01,\n",
      "         3.21433246e-01,  4.36361313e-01,  1.53183401e-01,\n",
      "        -1.59303129e-01],\n",
      "       [ 3.18033159e-01,  3.21116626e-01,  2.07713425e-01,\n",
      "        -1.32129192e-02, -1.34378076e-02,  3.73546362e-01,\n",
      "        -3.22183222e-01,  2.65912235e-01,  4.03069496e-01,\n",
      "        -9.41337943e-02],\n",
      "       [ 4.26528752e-01,  1.21916831e-01,  3.40414584e-01,\n",
      "         3.46786976e-02, -1.55671835e-01,  1.76371515e-01,\n",
      "        -1.28949881e-02,  3.00885677e-01,  2.62211204e-01,\n",
      "         3.41987014e-01],\n",
      "       [ 3.97052467e-01, -2.80063510e-01, -3.61421108e-02,\n",
      "         2.80022085e-01, -3.54739577e-01,  8.73158574e-02,\n",
      "         3.63785207e-01, -6.68840706e-02,  3.09062123e-01,\n",
      "        -1.41602278e-01],\n",
      "       [ 3.02354395e-01,  9.78669524e-02,  3.29984426e-01,\n",
      "         4.14924920e-01,  4.09927964e-01, -1.77690268e-01,\n",
      "        -2.83386946e-01, -3.47353220e-02,  1.44325435e-01,\n",
      "         3.05667341e-01],\n",
      "       [ 1.05843842e-01, -4.30595011e-01,  4.06636000e-01,\n",
      "        -1.46183878e-01,  3.46364379e-01, -4.52015996e-02,\n",
      "         3.21988523e-01, -3.50704968e-01, -3.57005626e-01,\n",
      "         3.46996546e-01],\n",
      "       [-4.97883260e-02, -1.66754633e-01,  1.25792623e-01,\n",
      "        -1.57859623e-02,  2.25221217e-02,  3.37444425e-01,\n",
      "         2.77411878e-01,  1.08788848e-01,  5.51053286e-02,\n",
      "        -1.55386120e-01],\n",
      "       [ 3.07020307e-01, -3.91896188e-01,  4.08881783e-01,\n",
      "        -2.71086693e-01, -3.39428961e-01, -4.16684777e-01,\n",
      "        -4.00727004e-01, -1.44044548e-01,  3.52397919e-01,\n",
      "         1.98519051e-01],\n",
      "       [-1.07016206e-01,  2.02765226e-01, -4.27497387e-01,\n",
      "         2.09332108e-01,  6.06575608e-02, -2.78723359e-01,\n",
      "        -7.28168786e-02, -3.10540944e-01, -3.02215815e-01,\n",
      "         9.21399593e-02],\n",
      "       [ 1.55206323e-01, -2.39079982e-01, -3.58897835e-01,\n",
      "        -1.32456481e-01,  3.01248193e-01, -1.40104890e-02,\n",
      "        -2.71280706e-02,  2.00005531e-01, -3.27920198e-01,\n",
      "        -3.89935881e-01]], dtype=float32)>, <tf.Variable 'dense_4/bias:0' shape=(10,) dtype=float32, numpy=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "# 13388 samples, 11 features as in our dataset\n",
    "input = tf.ones((5000, 21))\n",
    "# A fully-connected layer with 3 neurons\n",
    "layer = layers.Dense(10)\n",
    "# Calculate the outputs\n",
    "output = layer(input)\n",
    "# Print the weights\n",
    "print(layer.weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39abcdc6",
   "metadata": {},
   "source": [
    "<img src=\"Images/atom.png\" alt=\"Atom\" style=\"width:60px\" align=\"left\" vertical-align=\"middle\">\n",
    "\n",
    "## 6. Neural network model: input layer\n",
    "*in Machine Learning*\n",
    "\n",
    "----\n",
    "Inputs to a neural network are usually not considered the actual transformative layers. They are merely placeholders for data. In Keras, an input for a neural network can be specified with a `tf.keras.layers.InputLayer` object.\n",
    "\n",
    "<br/>The following code initializes an input layer for a `DataFrame` `my_data` that has 15 columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b8e9a5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import InputLayer\n",
    "my_input = InputLayer(input_shape=(15,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3f2f5e",
   "metadata": {},
   "source": [
    "Notice that the `input_shape` parameter has to have its first dimension equal to the number of features in the data. You don’t need to specify the second dimension: the number of samples or batch size.\n",
    "\n",
    "<br/>The following code avoids hard-coding with using the `.shape` property of the `my_data` DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "15ef44da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of features/dimensions in the data\n",
    "num_features = features.shape[1]\n",
    "# Without hard-coding\n",
    "my_input = tf.keras.layers.InputLayer(input_shape=(num_features,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ed1588",
   "metadata": {},
   "source": [
    "The following code adds this input layer to a model instance `my_model`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2063b88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model.add(my_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b199734",
   "metadata": {},
   "source": [
    "The following code prints a useful summary of a model instance `my_model`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "52f08d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my first model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      "=================================================================\n",
      "Total params: 0 (0.00 Byte)\n",
      "Trainable params: 0 (0.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(my_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddcf5b9",
   "metadata": {},
   "source": [
    "As you can see, the summary shows that the total number of parameters is 0. This shows you that the input layer has no trainable parameters and is just a placeholder for data.\n",
    "\n",
    "<br/>*Exercise:*\n",
    "1. In the `design_model()` function, create a variable called `num_features` and assign it the number of columns in the `features` `DataFrame` using the `.shape` property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a7053a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def design_model(features):\n",
    "    model = Sequential(name = \"my_first_model\")\n",
    "    # Your code here\n",
    "    num_features = features.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb7164e",
   "metadata": {},
   "source": [
    "2. In the `design_model()` function: create a variable called input, assign `input` an instance of `InputLayer`, set the first dimension of the `input_shape` parameter equal to `num_features`. Then add the `input` layer to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ee91ecbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def design_model(features):\n",
    "    model = Sequential(name = \"my_first_model\")\n",
    "    #your code here\n",
    "    num_features = features.shape[1]\n",
    "    input = layers.InputLayer(input_shape=(num_features,))\n",
    "    model.add(input)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e26899",
   "metadata": {},
   "source": [
    "3. Use the `.summary()` method to print the summary of the model instance model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dd725d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_first_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      "=================================================================\n",
      "Total params: 0 (0.00 Byte)\n",
      "Trainable params: 0 (0.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv('Data/insurance.csv') # Load the dataset\n",
    "features = dataset.iloc[:,0:6] # Choose first 7 columns as features\n",
    "labels = dataset.iloc[:,-1] # Choose the final column for prediction\n",
    "\n",
    "features = pd.get_dummies(features) # One-hot encoding for categorical variables\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(features, labels, test_size=0.33, random_state=42) # Split the data into training and test data\n",
    "\n",
    "# Standardize\n",
    "ct = ColumnTransformer([('standardize', StandardScaler(), ['age', 'bmi', 'children'])], remainder='passthrough')\n",
    "features_train = ct.fit_transform(features_train)\n",
    "features_test = ct.transform(features_test)\n",
    "\n",
    "# Invoke the function for our model design\n",
    "model = design_model(features_train)\n",
    "# Your code here\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b12c8d",
   "metadata": {},
   "source": [
    "<img src=\"Images/atom.png\" alt=\"Atom\" style=\"width:60px\" align=\"left\" vertical-align=\"middle\">\n",
    "\n",
    "## 7. Neural network model: output layer\n",
    "*in Machine Learning*\n",
    "\n",
    "----\n",
    "The output layer shape depends on your task. In the case of regression, we need one output for each sample, or, *one output for each prediction required.* For example, if your data has 100 samples, you would expect your output to be a vector with 100 entries - a numerical prediction for each sample.\n",
    "\n",
    "<br/>In our case, we are doing regression and wish to predict one number for each data point: the medical cost billed by health insurance indicated in the `charges` column in our data. Hence, our output layer has only one neuron.\n",
    "\n",
    "<br/>The following command adds a layer with one neuron to a model instance `my_model`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "11d1d7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "my_model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb44489",
   "metadata": {},
   "source": [
    "Notice that you don’t need to specify the input shape of this layer since Tensorflow with Keras can automatically infer its shape from the previous layer.\n",
    "\n",
    "<br/>*Exercise:*\n",
    "<br/>In a single command, create and add an output layer to the model instance model as an instance of `tensorflow.keras.layers.Dense`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bd727c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_first_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_6 (Dense)             (None, 1)                 12        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 12 (48.00 Byte)\n",
      "Trainable params: 12 (48.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def design_model(features):\n",
    "    model = Sequential(name = \"my_first_model\")\n",
    "    num_features = features.shape[1]\n",
    "    input = InputLayer(input_shape=(num_features,))\n",
    "    model.add(input) # Add the input layer\n",
    "    # Your code\n",
    "    model.add(Dense(1))\n",
    "    return model\n",
    "\n",
    "# Invoke the function for our model design\n",
    "model = design_model(features_train)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc2fa22",
   "metadata": {},
   "source": [
    "<img src=\"Images/atom.png\" alt=\"Atom\" style=\"width:60px\" align=\"left\" vertical-align=\"middle\">\n",
    "\n",
    "## 8. Neural network model: hidden layers\n",
    "*in Machine Learning*\n",
    "\n",
    "----\n",
    "So far we have added one input layer and one output layer to our model. If you think about it, our model currently represents a linear regression. To capture more complex or non-linear interactions among the inputs and outputs neural networks, we’ll need to incorporate hidden layers.\n",
    "\n",
    "<br/>The following command adds a hidden layer to a model instance `my_model`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5ffff230",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "my_model.add(Dense(64, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363ae076",
   "metadata": {},
   "source": [
    "We chose 64 (2<sup>6</sup>) to be the number of neurons since it makes optimization more efficient due to the binary nature of computation.\n",
    "\n",
    "<br/>With the `activation` parameter, we specify which activation function we want to have in the output of our hidden layer. There are a number of activation functions such as `softmax`, `sigmoid`, but `ReLU` (relu) (Rectified Linear Unit) is very effective in many applications and we’ll use it here.\n",
    "\n",
    "<br/>Adding more layers to a neural network naturally increases the number of parameters to be tuned. With every layer, there are associated weight and bias vectors.\n",
    "\n",
    "<br/>In the diagram below, we show the size of parameter vectors with each layer. In our case, the 1st layer’s weight matrix (red) has shape (11, 64) because we feed 11 features to 64 hidden neurons. The output layer (purple) has the weight matrix of shape (64, 1) because we have 64 input units and 1 neuron in the final layer.\n",
    "<img src=\"Images/hidden_layers_diagram.svg\" style=\"width:800px\">\n",
    "\n",
    "<br/>*Exercise:*\n",
    "<br/>In the `design_model()` function, in a single command, add a new hidden layer to the model instance model with the following parameters: 128 hidden units, a `relu` activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d16de971",
   "metadata": {},
   "outputs": [],
   "source": [
    "def design_model(features):\n",
    "    model = Sequential(name = \"my_first_model\")\n",
    "    input = InputLayer(input_shape=(features.shape[1],))\n",
    "    # Add the input layer\n",
    "    model.add(input) \n",
    "    # Add the hidden layer here\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    # Adding an output layer to our model\n",
    "    model.add(Dense(1)) \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3808fbca",
   "metadata": {},
   "source": [
    "<img src=\"Images/atom.png\" alt=\"Atom\" style=\"width:60px\" align=\"left\" vertical-align=\"middle\">\n",
    "\n",
    "## 9. Optimizers\n",
    "*in Machine Learning*\n",
    "\n",
    "----\n",
    "As we mentioned, our goal is for the network to effectively adjust its weights or parameters in order to reach the best performance. We do do this using *backpropagation,* which refers to the computation of gradients with an algorithm known as *gradient descent*. Keras offers a variety of optimizers such as `SGD` (Stochastic Gradient Descent optimizer), `Adam`, `RMSprop`, and others.\n",
    "\n",
    "<br/>We’ll start by introducing the Adam optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3fd800cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "opt = Adam(learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c693ea82",
   "metadata": {},
   "source": [
    "The learning rate determines how big of jumps the optimizer makes in the parameter space (weights and bias) and it is considered a *hyperparameter* that can be also tuned. While model parameters are the ones that the model uses to make predictions, hyperparameters determine the learning process (learning rate, number of iterations, optimizer type).\n",
    "\n",
    "<br/>If the learning rate is set too high, the optimizer will make large jumps and possibly miss the solution. On the other hand, if set too low, the learning process is too slow and might not converge to a desirable solution with the allotted time. Here we’ll use a value of 0.01, which is often used.\n",
    "\n",
    "<br/>Once the optimizer algorithm is chosen, a model instance `my_model` is compiled with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "900da42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model.compile(loss='mse',  metrics=['mae'], optimizer=opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56efd295",
   "metadata": {},
   "source": [
    "`loss` denotes the measure of learning success and the lower the loss the better the performance. In the case of regression, the most often used loss function is the Mean Squared Error `mse` (the average squared difference between the estimated values and the actual value).\n",
    "\n",
    "<br/>Additionally, we want to observe the progress of the Mean Absolute Error (`mae`) while training the model because MAE can give us a better idea than `mse` on how far off we are from the true values in the units we are predicting. In our case, we are predicting `charge` in dollars and MAE will tell us how many dollars we’re off, on average, from the actual values as the network is being trained.\n",
    "\n",
    "<br/>*Exercise:*\n",
    "<br/>In the `design_model()` function, create an instance of `Adam` optimizer with 0.01 learning rate and assign the result to a variable called `opt`. Then, in the `design_model()` function, use the `.compile()` method to compile the model instance model with: the `mse` loss, `mae` metrics, `opt` as the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dce88010",
   "metadata": {},
   "outputs": [],
   "source": [
    "def design_model(features):\n",
    "    model = Sequential(name = \"my_first_model\")\n",
    "    # Add the input layer\n",
    "    input = InputLayer(input_shape=(features.shape[1],))\n",
    "    model.add(input)\n",
    "    # Add a hidden layer with 128 neurons\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    # Add an output layer\n",
    "    model.add(Dense(1))\n",
    "    # Add the Adam optimizer\n",
    "    opt = Adam(learning_rate=0.01)\n",
    "    model.compile(loss='mse', metrics=['mae'], optimizer=opt)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb59149",
   "metadata": {},
   "source": [
    "<img src=\"Images/atom.png\" alt=\"Atom\" style=\"width:60px\" align=\"left\" vertical-align=\"middle\">\n",
    "\n",
    "## 10. Training and evaluating the model\n",
    "*in Machine Learning*\n",
    "\n",
    "----\n",
    "Now that we built the model we are ready to train the model using the training data.\n",
    "\n",
    "<br/>The following command trains a model instance `my_model` using training data `my_data` and training labels `my_labels`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8517d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model.fit(my_data, my_labels, epochs=50, batch_size=3, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1843bc55",
   "metadata": {},
   "source": [
    "`model.fit()` takes the following parameters:\n",
    "- `my_data` is the training data set.\n",
    "- `my_labels` are true labels for the training data points.\n",
    "- `epochs` refers to the number of cycles through the full training dataset. Since training of neural networks is an iterative process, you need multiple passes through data. Here we chose 50 epochs, but how do you pick a number of epochs? Well, it is hard to give one answer since it depends on your dataset. Amongst others, this is a hyperparameter that can be tuned — which we’ll cover later.\n",
    "- `batch_size` is the number of data points to work through before updating the model parameters. It is also a hyperparameter that can be tuned.\n",
    "- `verbose = 1` will show you the progress bar of the training.\n",
    "\n",
    "<br/>When the training is finalized, we use the trained model to predict values for samples that the training procedure haven’t seen: the *test set*.\n",
    "\n",
    "<br/>The following commands evaluates the model instance `my_model` using the test data `my_data` and test labels `my_labels`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bd6548",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_mse, val_mae = my_model.evaluate(my_data, my_labels, verbose = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b927dbe",
   "metadata": {},
   "source": [
    "In our case, `model.evaluate()` returns the value for our chosen loss metrics (`mse`) and for an additional metrics (`mae`).\n",
    "\n",
    "<br/>So what is the final result? We should get ~$3884.21. This means that on average we’re off with our prediction by around 3800 dollars. Is that a good result or a bad result?\n",
    "\n",
    "<br/>Often you need an expert or domain knowledge to decide this. What is an acceptable error for the application? Is $3800 a big error when deciding on insurance charges? Can you do better and how? As you see, the process doesn’t stop here.\n",
    "\n",
    "<br/>*Exercise:*\n",
    "1. Using the `.fit()` method, train the model instance model with: the training data `features_train`, training labels `labels_train`, 40 epochs, batch size equal to 1, verbose set to true (1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8222c22c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "WARNING:tensorflow:From C:\\Users\\Appuk\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Appuk\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "896/896 [==============================] - 1s 754us/step - loss: 224376080.0000 - mae: 9875.2764\n",
      "Epoch 2/40\n",
      "896/896 [==============================] - 1s 722us/step - loss: 104466376.0000 - mae: 7393.5312\n",
      "Epoch 3/40\n",
      "896/896 [==============================] - 1s 727us/step - loss: 61116964.0000 - mae: 5496.4312\n",
      "Epoch 4/40\n",
      "896/896 [==============================] - 1s 739us/step - loss: 42273292.0000 - mae: 4672.7412\n",
      "Epoch 5/40\n",
      "896/896 [==============================] - 1s 738us/step - loss: 37968948.0000 - mae: 4282.5317\n",
      "Epoch 6/40\n",
      "896/896 [==============================] - 1s 743us/step - loss: 36877816.0000 - mae: 4226.6206\n",
      "Epoch 7/40\n",
      "896/896 [==============================] - 1s 750us/step - loss: 36377488.0000 - mae: 4107.9478\n",
      "Epoch 8/40\n",
      "896/896 [==============================] - 1s 734us/step - loss: 35837896.0000 - mae: 4063.3865\n",
      "Epoch 9/40\n",
      "896/896 [==============================] - 1s 747us/step - loss: 35204308.0000 - mae: 3980.6829\n",
      "Epoch 10/40\n",
      "896/896 [==============================] - 1s 730us/step - loss: 34449220.0000 - mae: 3915.1658\n",
      "Epoch 11/40\n",
      "896/896 [==============================] - 1s 756us/step - loss: 33715888.0000 - mae: 3873.1836\n",
      "Epoch 12/40\n",
      "896/896 [==============================] - 1s 758us/step - loss: 33110144.0000 - mae: 3808.9456\n",
      "Epoch 13/40\n",
      "896/896 [==============================] - 1s 754us/step - loss: 32261996.0000 - mae: 3704.0454\n",
      "Epoch 14/40\n",
      "896/896 [==============================] - 1s 771us/step - loss: 31670370.0000 - mae: 3714.6699\n",
      "Epoch 15/40\n",
      "896/896 [==============================] - 1s 744us/step - loss: 30965344.0000 - mae: 3601.5503\n",
      "Epoch 16/40\n",
      "896/896 [==============================] - 1s 734us/step - loss: 30282306.0000 - mae: 3545.2058\n",
      "Epoch 17/40\n",
      "896/896 [==============================] - 1s 721us/step - loss: 29507132.0000 - mae: 3536.1387\n",
      "Epoch 18/40\n",
      "896/896 [==============================] - 1s 725us/step - loss: 28990464.0000 - mae: 3403.7014\n",
      "Epoch 19/40\n",
      "896/896 [==============================] - 1s 744us/step - loss: 28342366.0000 - mae: 3387.7148\n",
      "Epoch 20/40\n",
      "896/896 [==============================] - 1s 741us/step - loss: 27832384.0000 - mae: 3346.5273\n",
      "Epoch 21/40\n",
      "896/896 [==============================] - 1s 731us/step - loss: 27294270.0000 - mae: 3302.0364\n",
      "Epoch 22/40\n",
      "896/896 [==============================] - 1s 745us/step - loss: 26868108.0000 - mae: 3179.1899\n",
      "Epoch 23/40\n",
      "896/896 [==============================] - 1s 749us/step - loss: 26471162.0000 - mae: 3230.6516\n",
      "Epoch 24/40\n",
      "896/896 [==============================] - 1s 747us/step - loss: 26005136.0000 - mae: 3067.3433\n",
      "Epoch 25/40\n",
      "896/896 [==============================] - 1s 753us/step - loss: 25852468.0000 - mae: 3137.3503\n",
      "Epoch 26/40\n",
      "896/896 [==============================] - 1s 766us/step - loss: 25431328.0000 - mae: 3099.3086\n",
      "Epoch 27/40\n",
      "896/896 [==============================] - 1s 763us/step - loss: 25218074.0000 - mae: 3024.0078\n",
      "Epoch 28/40\n",
      "896/896 [==============================] - 1s 737us/step - loss: 24957210.0000 - mae: 3029.1907\n",
      "Epoch 29/40\n",
      "896/896 [==============================] - 1s 743us/step - loss: 24747366.0000 - mae: 3003.7241\n",
      "Epoch 30/40\n",
      "896/896 [==============================] - 1s 731us/step - loss: 24599872.0000 - mae: 2989.7566\n",
      "Epoch 31/40\n",
      "896/896 [==============================] - 1s 751us/step - loss: 24358122.0000 - mae: 2940.3848\n",
      "Epoch 32/40\n",
      "896/896 [==============================] - 1s 726us/step - loss: 24187846.0000 - mae: 2983.9597\n",
      "Epoch 33/40\n",
      "896/896 [==============================] - 1s 724us/step - loss: 24143968.0000 - mae: 2910.5867\n",
      "Epoch 34/40\n",
      "896/896 [==============================] - 1s 748us/step - loss: 23992996.0000 - mae: 2986.9089\n",
      "Epoch 35/40\n",
      "896/896 [==============================] - 1s 726us/step - loss: 23962820.0000 - mae: 2921.0627\n",
      "Epoch 36/40\n",
      "896/896 [==============================] - 1s 761us/step - loss: 23814284.0000 - mae: 2943.4473\n",
      "Epoch 37/40\n",
      "896/896 [==============================] - 1s 782us/step - loss: 23703456.0000 - mae: 2886.4075\n",
      "Epoch 38/40\n",
      "896/896 [==============================] - 1s 752us/step - loss: 23530798.0000 - mae: 2911.4351\n",
      "Epoch 39/40\n",
      "896/896 [==============================] - 1s 748us/step - loss: 23625684.0000 - mae: 2903.8823\n",
      "Epoch 40/40\n",
      "896/896 [==============================] - 1s 745us/step - loss: 23464352.0000 - mae: 2918.1167\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2d4ff76fc10>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = design_model(features_train)\n",
    "# Fit the model using 40 epochs and batch size 1\n",
    "model.fit(features_train, labels_train, epochs=40, batch_size=1, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a18a0c6",
   "metadata": {},
   "source": [
    "2. Using the .evaluate() method, evaluate the model instance model with: the test data `features_test`, test labels `labels_test`, the verbose parameter set to false (0). Assign the result to variables `val_mse` and `val_mae`, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9da3e0ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:  2632.363037109375\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test data\n",
    "val_mse, val_mae = model.evaluate(features_test, labels_test, verbose=0)\n",
    "print(\"MAE: \", val_mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad81801a",
   "metadata": {},
   "source": [
    "<img src=\"Images/atom.png\" alt=\"Atom\" style=\"width:60px\" align=\"left\" vertical-align=\"middle\">\n",
    "\n",
    "## 11. Summary\n",
    "*of implementing Neural Networks*\n",
    "\n",
    "----\n",
    "Congrats! You have built your neural network, trained it, and evaluated it using TensorFlow with Keras. To remind you, these are the concepts you learned in this lesson:\n",
    "\n",
    "<br/>A. Preparing the data for learning:\n",
    "- separating features from labels using array slicing\n",
    "- determining the shape of your data\n",
    "- preprocessing the categorical variables using one-hot encoding\n",
    "- splitting the data into training and test sets\n",
    "- scaling the numerical features\n",
    "\n",
    "<br/>B. Designing a Sequential model by chaining `InputLayer()` and the `tf.keras.layers.Dense layers`. `InputLayer()` was used as a placeholder for the input data. The output layer in this case needed one neuron since we need a prediction of a single value in the regression. And finally, hidden layers were added with the `relu` activation function to handle complex dependencies in the data.\n",
    "\n",
    "<br/>C. Choosing an optimizer using `keras.optimizers` with a specific learning rate hyperparameter.\n",
    "\n",
    "<br/>D. Training the model - using `model.fit()` to train the model on the training data and training labels.\n",
    "\n",
    "<br/>E. Setting the values for the learning hyperparameters: number of epochs and batch sizes.\n",
    "\n",
    "<br/>F. Evaluating the model using `model.evaluate()` on the test data.\n",
    "\n",
    "<br/>You might be wondering, what do I do with the plethora of hyperparameters? Or why if I use different random states I receive different results? Plus, how I can guarantee that my good performance isn’t just good luck? And you are right! This is not the full story. In machine learning, we tweak the hyperparameters using a better evaluation methodology — something we’ll cover next.\n",
    "\n",
    "<br/>*Exercise: put it all together!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3df2bf9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Appuk\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Appuk\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "MAE:  2718.997802734375\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import Normalizer, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import InputLayer, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "'''A. IMPORT DATA'''\n",
    "dataset = pd.read_csv('Data/insurance.csv') # 1. Import the data as a pandas dataframe.\n",
    "features = dataset.iloc[:,0:6]  # 2. Dataframe slicing using iloc, this designates the first 6 rows as features (independent variables).\n",
    "labels = dataset.iloc[:,-1] # 3. We select the last column with -1, which is designated as target variables (dependent variable).\n",
    "\n",
    "'''B. DATA PRE-PROCESSING'''\n",
    "features = pd.get_dummies(features) # 4. One-hot encoding for categorical variables.\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(features, labels, test_size=0.33, random_state=42) # 5. Split the data into training (67%) and test (33%) data.\n",
    "# ct = ColumnTransformer([('normalize', Normalizer(), ['age', 'bmi', 'children'])], remainder='passthrough') # 6a. Create a new ColumnTransformer instance and normalize the data (scale the numerical features to a fixed range, usually between 0 and 1).\n",
    "ct = ColumnTransformer([('standardize', StandardScaler(), ['age', 'bmi', 'children'])], remainder='passthrough') # 6b. You may also standardize the data (rescale features to zero mean and unit variance).\n",
    "features_train = ct.fit_transform(features_train) # 7. Fit the column transformer to the features_train DataFrame.\n",
    "features_test = ct.transform(features_test) # 8. Transform the trained column transformer my_ct to the features_test DataFrame.\n",
    "\n",
    "'''C. CREATE THE MODEL'''\n",
    "def design_model(features):\n",
    "    model = Sequential(name = \"My_Sequential_Model\") # 9. The most frequently used model in TensorFlow is Keras \"Sequential\". A sequential model allows us to create models layer-by-layer in a step-by-step fashion. \n",
    "    model.add(InputLayer(input_shape=(features.shape[1],))) # 10. Add the input layer. Notice that the input_shape parameter has to have its first dimension equal to the number of features (features.shape[1]) in the data. You don’t need to specify the second dimension: the number of samples or batch size.\n",
    "    model.add(Dense(128, activation='relu')) # 11. Add a hidden layer with 128 neurons. With the activation parameter, we specify which activation function (ReLu) we want to have in the output of our hidden layer.\n",
    "    model.add(Dense(1)) # 12. Add an output layer. The output layer shape depends on your task. In the case of regression, we need one output for each prediction required.\n",
    "    opt = Adam(learning_rate=0.01) # 13. Optimize the model's weights using backpropagation, which refers to the computation of gradients with an algorithm known as gradient descent. Keras offers a variety of optimizers: SGD, Adam, RMSprop, and others. Also determine the learning rate.\n",
    "    model.compile(loss='mse', metrics=['mae'], optimizer=opt) # 14. Compile an instance of the model. \"loss\" denotes the measure of learning success and is inversely proportional to performance. The most commonly used loss function is the Mean Squared Error (mse).\n",
    "    return model\n",
    "\n",
    "'''D. TRAIN THE MODEL'''\n",
    "model = design_model(features_train) # 15. Construct the model\n",
    "model.fit(features_train, labels_train, epochs=50, batch_size=1, verbose=0) # 16. Fit the model using 50 epochs and batch size 1.\n",
    "\n",
    "'''E. TEST THE MODEL'''\n",
    "val_mse, val_mae = model.evaluate(features_test, labels_test, verbose=0) # 17. Evaluate the model on the test data.\n",
    "print(\"MAE: \", val_mae)  # 18. We want to observe the progress of the Mean Absolute Error (mae) while training the model because MAE can give us a better idea than mse on how far off we are from the true values in the units we are predicting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
